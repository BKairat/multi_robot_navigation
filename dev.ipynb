{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from policies import MultiRobotNavigation\n",
    "from history import History\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = [History({\n",
    "    \"ol\": torch.randn(24),\n",
    "    \"ov\": torch.randn(2), \n",
    "    \"og\": torch.randn(2),\n",
    "    \"od\": torch.randn(1)\n",
    "    }) for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MultiRobotNavigation(History({\n",
    "    \"ol\": torch.randn(24),\n",
    "    \"ov\": torch.randn(2), \n",
    "    \"og\": torch.randn(2),\n",
    "    \"od\": torch.randn(1)\n",
    "    }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m m\u001b[38;5;241m.\u001b[39mforward(h)\n",
      "File \u001b[0;32m~/Desktop/mutlti_robots/policies/policies.py:86\u001b[0m, in \u001b[0;36mMultiRobotNavigation.forward\u001b[0;34m(self, history)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, history):\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;66;03m# Apply 1D convolution to ol\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m     obs \u001b[38;5;241m=\u001b[39m history\u001b[38;5;241m.\u001b[39mget()\n\u001b[1;32m     88\u001b[0m     ol \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(obs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mol\u001b[39m\u001b[38;5;124m\"\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     89\u001b[0m     od \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(obs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mod\u001b[39m\u001b[38;5;124m\"\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "m.forward(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $$\\log \\pi_\\theta(a_t | s_t)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.11.5)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([[<history.History at 0x1615a9a50>],\n",
       "  [<history.History at 0x14cab05d0>],\n",
       "  [<history.History at 0x146e9af90>],\n",
       "  [<history.History at 0x146709490>],\n",
       "  [<history.History at 0x146ea9e90>],\n",
       "  [<history.History at 0x16180bc10>],\n",
       "  [<history.History at 0x161819790>],\n",
       "  [<history.History at 0x161819ad0>],\n",
       "  [<history.History at 0x161819d10>],\n",
       "  [<history.History at 0x161819fd0>],\n",
       "  [<history.History at 0x16181a250>],\n",
       "  [<history.History at 0x16181a550>],\n",
       "  [<history.History at 0x16181a850>],\n",
       "  [<history.History at 0x16181ab90>],\n",
       "  [<history.History at 0x16181ad90>],\n",
       "  [<history.History at 0x16181b0d0>],\n",
       "  [<history.History at 0x16181b410>],\n",
       "  [<history.History at 0x16181b690>],\n",
       "  [<history.History at 0x16181b950>],\n",
       "  [<history.History at 0x16181bc50>],\n",
       "  [<history.History at 0x16181bf10>],\n",
       "  [<history.History at 0x161830250>],\n",
       "  [<history.History at 0x161830550>],\n",
       "  [<history.History at 0x10c1c7890>],\n",
       "  [<history.History at 0x161830b10>],\n",
       "  [<history.History at 0x161830dd0>],\n",
       "  [<history.History at 0x161831090>],\n",
       "  [<history.History at 0x161831350>],\n",
       "  [<history.History at 0x161831610>],\n",
       "  [<history.History at 0x161819890>],\n",
       "  [<history.History at 0x161831b10>],\n",
       "  [<history.History at 0x161819d50>],\n",
       "  [<history.History at 0x161831f90>],\n",
       "  [<history.History at 0x161832210>],\n",
       "  [<history.History at 0x16180a410>],\n",
       "  [<history.History at 0x16181a650>],\n",
       "  [<history.History at 0x1618328d0>],\n",
       "  [<history.History at 0x161832b10>],\n",
       "  [<history.History at 0x16181ad50>],\n",
       "  [<history.History at 0x161832f50>],\n",
       "  [<history.History at 0x16181b150>],\n",
       "  [<history.History at 0x161833450>],\n",
       "  [<history.History at 0x16181b650>],\n",
       "  [<history.History at 0x161833910>],\n",
       "  [<history.History at 0x16181bb10>],\n",
       "  [<history.History at 0x161833d90>],\n",
       "  [<history.History at 0x161830050>],\n",
       "  [<history.History at 0x1615aa050>],\n",
       "  [<history.History at 0x161830410>],\n",
       "  [<history.History at 0x161830610>],\n",
       "  [<history.History at 0x161830850>],\n",
       "  [<history.History at 0x161838b50>],\n",
       "  [<history.History at 0x161830d90>],\n",
       "  [<history.History at 0x161839050>],\n",
       "  [<history.History at 0x161839310>],\n",
       "  [<history.History at 0x161839550>],\n",
       "  [<history.History at 0x161819750>],\n",
       "  [<history.History at 0x161839990>],\n",
       "  [<history.History at 0x161819b50>],\n",
       "  [<history.History at 0x161839d50>],\n",
       "  [<history.History at 0x161831f50>],\n",
       "  [<history.History at 0x16183a1d0>],\n",
       "  [<history.History at 0x16181a210>],\n",
       "  [<history.History at 0x161832590>],\n",
       "  [<history.History at 0x16183a7d0>],\n",
       "  [<history.History at 0x14dd9a9d0>],\n",
       "  [<history.History at 0x16183ac10>],\n",
       "  [<history.History at 0x16183ae90>],\n",
       "  [<history.History at 0x16183b110>],\n",
       "  [<history.History at 0x16183b350>],\n",
       "  [<history.History at 0x14cab35d0>],\n",
       "  [<history.History at 0x16183b810>],\n",
       "  [<history.History at 0x16181ba10>],\n",
       "  [<history.History at 0x161833c90>],\n",
       "  [<history.History at 0x16181bed0>],\n",
       "  [<history.History at 0x1618440d0>],\n",
       "  [<history.History at 0x146712ad0>],\n",
       "  [<history.History at 0x1618445d0>],\n",
       "  [<history.History at 0x1615ab810>],\n",
       "  [<history.History at 0x1424a8f90>],\n",
       "  [<history.History at 0x161844cd0>],\n",
       "  [<history.History at 0x16183b150>],\n",
       "  [<history.History at 0x161845210>],\n",
       "  [<history.History at 0x161845490>],\n",
       "  [<history.History at 0x161845790>],\n",
       "  [<history.History at 0x161845a50>],\n",
       "  [<history.History at 0x161832450>],\n",
       "  [<history.History at 0x161832250>],\n",
       "  [<history.History at 0x161831fd0>],\n",
       "  [<history.History at 0x161846350>],\n",
       "  [<history.History at 0x161846590>],\n",
       "  [<history.History at 0x1618467d0>],\n",
       "  [<history.History at 0x161846a90>],\n",
       "  [<history.History at 0x161846d90>],\n",
       "  [<history.History at 0x161846f90>],\n",
       "  [<history.History at 0x161838dd0>],\n",
       "  [<history.History at 0x1618474d0>],\n",
       "  [<history.History at 0x1618477d0>],\n",
       "  [<history.History at 0x161838650>],\n",
       "  [<history.History at 0x161847cd0>],\n",
       "  [<history.History at 0x161847fd0>],\n",
       "  [<history.History at 0x161844290>],\n",
       "  [<history.History at 0x161844450>],\n",
       "  [<history.History at 0x161844650>],\n",
       "  [<history.History at 0x16184c8d0>],\n",
       "  [<history.History at 0x161844a90>],\n",
       "  [<history.History at 0x161844c90>],\n",
       "  [<history.History at 0x16181b110>],\n",
       "  [<history.History at 0x16184d190>],\n",
       "  [<history.History at 0x16184d410>],\n",
       "  [<history.History at 0x161845590>],\n",
       "  [<history.History at 0x16181a890>],\n",
       "  [<history.History at 0x161845a10>],\n",
       "  [<history.History at 0x14cad5c90>],\n",
       "  [<history.History at 0x161845e90>],\n",
       "  [<history.History at 0x14671e0d0>],\n",
       "  [<history.History at 0x161846250>],\n",
       "  [<history.History at 0x16184e490>],\n",
       "  [<history.History at 0x16184e710>],\n",
       "  [<history.History at 0x16184ea10>],\n",
       "  [<history.History at 0x161846bd0>],\n",
       "  [<history.History at 0x161846e10>],\n",
       "  [<history.History at 0x161847010>],\n",
       "  [<history.History at 0x16184f250>],\n",
       "  [<history.History at 0x16184f4d0>],\n",
       "  [<history.History at 0x16184f690>],\n",
       "  [<history.History at 0x16184f8d0>],\n",
       "  [<history.History at 0x16184fa90>],\n",
       "  [<history.History at 0x16184fcd0>],\n",
       "  [<history.History at 0x16184fe90>],\n",
       "  [<history.History at 0x161858090>],\n",
       "  [<history.History at 0x161858250>],\n",
       "  [<history.History at 0x161858410>],\n",
       "  [<history.History at 0x1618585d0>],\n",
       "  [<history.History at 0x161858850>],\n",
       "  [<history.History at 0x161858a50>],\n",
       "  [<history.History at 0x161858c10>],\n",
       "  [<history.History at 0x161858e50>],\n",
       "  [<history.History at 0x161859050>],\n",
       "  [<history.History at 0x161859210>],\n",
       "  [<history.History at 0x161859410>],\n",
       "  [<history.History at 0x161859610>],\n",
       "  [<history.History at 0x1618597d0>],\n",
       "  [<history.History at 0x161859990>],\n",
       "  [<history.History at 0x161859b50>],\n",
       "  [<history.History at 0x161859c90>],\n",
       "  [<history.History at 0x161859ed0>],\n",
       "  [<history.History at 0x16185a090>],\n",
       "  [<history.History at 0x16185a250>],\n",
       "  [<history.History at 0x16185a410>],\n",
       "  [<history.History at 0x16185a610>],\n",
       "  [<history.History at 0x16185a7d0>],\n",
       "  [<history.History at 0x16185a9d0>],\n",
       "  [<history.History at 0x16185aad0>],\n",
       "  [<history.History at 0x16185acd0>],\n",
       "  [<history.History at 0x16185ae90>],\n",
       "  [<history.History at 0x16185b050>],\n",
       "  [<history.History at 0x16185b210>],\n",
       "  [<history.History at 0x16185b410>],\n",
       "  [<history.History at 0x16185b5d0>],\n",
       "  [<history.History at 0x16185b7d0>],\n",
       "  [<history.History at 0x16185b990>],\n",
       "  [<history.History at 0x16185bb90>],\n",
       "  [<history.History at 0x16185bdd0>],\n",
       "  [<history.History at 0x16185bfd0>],\n",
       "  [<history.History at 0x1618641d0>],\n",
       "  [<history.History at 0x1618643d0>],\n",
       "  [<history.History at 0x161864610>],\n",
       "  [<history.History at 0x1618647d0>],\n",
       "  [<history.History at 0x161864990>],\n",
       "  [<history.History at 0x161864b50>],\n",
       "  [<history.History at 0x161864d90>],\n",
       "  [<history.History at 0x161864f10>],\n",
       "  [<history.History at 0x1618650d0>],\n",
       "  [<history.History at 0x161865310>],\n",
       "  [<history.History at 0x1618654d0>],\n",
       "  [<history.History at 0x1618656d0>],\n",
       "  [<history.History at 0x161865890>],\n",
       "  [<history.History at 0x161865a90>],\n",
       "  [<history.History at 0x161865c50>],\n",
       "  [<history.History at 0x161865e50>],\n",
       "  [<history.History at 0x161866050>],\n",
       "  [<history.History at 0x1618662d0>],\n",
       "  [<history.History at 0x1618664d0>],\n",
       "  [<history.History at 0x161866690>],\n",
       "  [<history.History at 0x161866850>],\n",
       "  [<history.History at 0x161866a10>],\n",
       "  [<history.History at 0x161866bd0>],\n",
       "  [<history.History at 0x161866d90>],\n",
       "  [<history.History at 0x161866f50>],\n",
       "  [<history.History at 0x161867150>],\n",
       "  [<history.History at 0x161867390>],\n",
       "  [<history.History at 0x161867590>],\n",
       "  [<history.History at 0x161867750>],\n",
       "  [<history.History at 0x161867910>],\n",
       "  [<history.History at 0x161867b10>],\n",
       "  [<history.History at 0x161867cd0>],\n",
       "  [<history.History at 0x161867e90>],\n",
       "  [<history.History at 0x16186c090>],\n",
       "  [<history.History at 0x16186c250>],\n",
       "  [<history.History at 0x16186c410>],\n",
       "  [<history.History at 0x16186c5d0>],\n",
       "  [<history.History at 0x16186c790>],\n",
       "  [<history.History at 0x16186c990>],\n",
       "  [<history.History at 0x16186cb50>],\n",
       "  [<history.History at 0x16186cd10>],\n",
       "  [<history.History at 0x16186ced0>],\n",
       "  [<history.History at 0x16186d090>],\n",
       "  [<history.History at 0x16186d250>],\n",
       "  [<history.History at 0x16186d410>],\n",
       "  [<history.History at 0x16186d610>],\n",
       "  [<history.History at 0x16186d7d0>],\n",
       "  [<history.History at 0x16186d990>],\n",
       "  [<history.History at 0x16186db50>],\n",
       "  [<history.History at 0x16186dd50>],\n",
       "  [<history.History at 0x16186df10>],\n",
       "  [<history.History at 0x16186e0d0>],\n",
       "  [<history.History at 0x16186e2d0>],\n",
       "  [<history.History at 0x16186e490>],\n",
       "  [<history.History at 0x16186e690>],\n",
       "  [<history.History at 0x16186e850>],\n",
       "  [<history.History at 0x16186ea10>],\n",
       "  [<history.History at 0x16186ebd0>],\n",
       "  [<history.History at 0x16186ed90>],\n",
       "  [<history.History at 0x16186ef50>],\n",
       "  [<history.History at 0x16186f110>],\n",
       "  [<history.History at 0x16186f310>],\n",
       "  [<history.History at 0x16186f4d0>],\n",
       "  [<history.History at 0x16186f690>],\n",
       "  [<history.History at 0x16186f810>],\n",
       "  [<history.History at 0x16186fa10>],\n",
       "  [<history.History at 0x16186fbd0>],\n",
       "  [<history.History at 0x16186fdd0>],\n",
       "  [<history.History at 0x16186fe50>],\n",
       "  [<history.History at 0x161878190>],\n",
       "  [<history.History at 0x161878390>],\n",
       "  [<history.History at 0x161878550>],\n",
       "  [<history.History at 0x161878750>],\n",
       "  [<history.History at 0x1618788d0>],\n",
       "  [<history.History at 0x161878ad0>],\n",
       "  [<history.History at 0x161878cd0>],\n",
       "  [<history.History at 0x161878e90>],\n",
       "  [<history.History at 0x161879090>],\n",
       "  [<history.History at 0x161879210>],\n",
       "  [<history.History at 0x1618793d0>],\n",
       "  [<history.History at 0x161879590>],\n",
       "  [<history.History at 0x161879750>],\n",
       "  [<history.History at 0x161879910>],\n",
       "  [<history.History at 0x161879b10>],\n",
       "  [<history.History at 0x161879c90>],\n",
       "  [<history.History at 0x161879e50>],\n",
       "  [<history.History at 0x16187a010>],\n",
       "  [<history.History at 0x16187a1d0>],\n",
       "  [<history.History at 0x16187a210>],\n",
       "  [<history.History at 0x16187a590>],\n",
       "  [<history.History at 0x16187a710>],\n",
       "  [<history.History at 0x16187a8d0>]],\n",
       " tensor([[[1.0000e+00, 2.2341e-04]],\n",
       " \n",
       "         [[1.0000e+00, 1.7547e-04]],\n",
       " \n",
       "         [[1.0000e+00, 2.3532e-04]],\n",
       " \n",
       "         [[1.0000e+00, 2.0503e-04]],\n",
       " \n",
       "         [[1.0000e+00, 7.4619e-04]],\n",
       " \n",
       "         [[1.0000e+00, 1.1751e-03]],\n",
       " \n",
       "         [[1.0000e+00, 1.7012e-03]],\n",
       " \n",
       "         [[1.0000e+00, 8.3848e-04]],\n",
       " \n",
       "         [[1.0000e+00, 6.1669e-05]],\n",
       " \n",
       "         [[1.0000e+00, 1.9766e-05]],\n",
       " \n",
       "         [[1.0000e+00, 4.6477e-05]],\n",
       " \n",
       "         [[1.0000e+00, 6.0582e-04]],\n",
       " \n",
       "         [[1.0000e+00, 5.7227e-05]],\n",
       " \n",
       "         [[1.0000e+00, 1.6891e-04]],\n",
       " \n",
       "         [[1.0000e+00, 8.3065e-05]],\n",
       " \n",
       "         [[1.0000e+00, 1.2606e-04]],\n",
       " \n",
       "         [[1.0000e+00, 1.7464e-04]],\n",
       " \n",
       "         [[1.0000e+00, 2.2684e-04]],\n",
       " \n",
       "         [[1.0000e+00, 3.7465e-04]],\n",
       " \n",
       "         [[1.0000e+00, 1.1956e-04]],\n",
       " \n",
       "         [[1.0000e+00, 1.8531e-04]],\n",
       " \n",
       "         [[1.0000e+00, 9.8581e-05]],\n",
       " \n",
       "         [[1.0000e+00, 9.8104e-05]],\n",
       " \n",
       "         [[1.0000e+00, 3.4297e-05]],\n",
       " \n",
       "         [[1.0000e+00, 2.2163e-04]],\n",
       " \n",
       "         [[1.0000e+00, 1.8091e-04]],\n",
       " \n",
       "         [[1.0000e+00, 2.4070e-04]],\n",
       " \n",
       "         [[1.0000e+00, 2.0287e-04]],\n",
       " \n",
       "         [[1.0000e+00, 7.4410e-04]],\n",
       " \n",
       "         [[1.0000e+00, 1.1737e-03]],\n",
       " \n",
       "         [[1.0000e+00, 1.6557e-03]],\n",
       " \n",
       "         [[1.0000e+00, 8.6896e-04]],\n",
       " \n",
       "         [[1.0000e+00, 6.0572e-05]],\n",
       " \n",
       "         [[1.0000e+00, 1.9782e-05]],\n",
       " \n",
       "         [[1.0000e+00, 4.4864e-05]],\n",
       " \n",
       "         [[1.0000e+00, 6.1500e-04]],\n",
       " \n",
       "         [[1.0000e+00, 5.6566e-05]],\n",
       " \n",
       "         [[1.0000e+00, 1.9871e-04]],\n",
       " \n",
       "         [[1.0000e+00, 8.3304e-05]],\n",
       " \n",
       "         [[1.0000e+00, 1.2558e-04]],\n",
       " \n",
       "         [[1.0000e+00, 1.7576e-04]],\n",
       " \n",
       "         [[1.0000e+00, 2.2673e-04]],\n",
       " \n",
       "         [[1.0000e+00, 3.7468e-04]],\n",
       " \n",
       "         [[1.0000e+00, 1.1899e-04]],\n",
       " \n",
       "         [[1.0000e+00, 1.8395e-04]],\n",
       " \n",
       "         [[1.0000e+00, 9.8486e-05]],\n",
       " \n",
       "         [[1.0000e+00, 1.0062e-04]],\n",
       " \n",
       "         [[1.0000e+00, 3.4917e-05]],\n",
       " \n",
       "         [[1.0000e+00, 2.1590e-04]],\n",
       " \n",
       "         [[1.0000e+00, 1.8886e-04]],\n",
       " \n",
       "         [[1.0000e+00, 2.4853e-04]],\n",
       " \n",
       "         [[1.0000e+00, 1.9848e-04]],\n",
       " \n",
       "         [[1.0000e+00, 7.4799e-04]],\n",
       " \n",
       "         [[1.0000e+00, 1.1565e-03]],\n",
       " \n",
       "         [[1.0000e+00, 1.6042e-03]],\n",
       " \n",
       "         [[1.0000e+00, 8.6967e-04]],\n",
       " \n",
       "         [[1.0000e+00, 6.1699e-05]],\n",
       " \n",
       "         [[1.0000e+00, 1.9863e-05]],\n",
       " \n",
       "         [[1.0000e+00, 4.4224e-05]],\n",
       " \n",
       "         [[1.0000e+00, 5.9612e-04]],\n",
       " \n",
       "         [[1.0000e+00, 5.5598e-05]],\n",
       " \n",
       "         [[1.0000e+00, 1.9886e-04]],\n",
       " \n",
       "         [[1.0000e+00, 8.3991e-05]],\n",
       " \n",
       "         [[1.0000e+00, 1.2437e-04]],\n",
       " \n",
       "         [[1.0000e+00, 1.7594e-04]],\n",
       " \n",
       "         [[1.0000e+00, 2.2671e-04]],\n",
       " \n",
       "         [[1.0000e+00, 3.7258e-04]],\n",
       " \n",
       "         [[1.0000e+00, 1.1739e-04]],\n",
       " \n",
       "         [[1.0000e+00, 1.8316e-04]],\n",
       " \n",
       "         [[1.0000e+00, 9.8176e-05]],\n",
       " \n",
       "         [[1.0000e+00, 1.0594e-04]],\n",
       " \n",
       "         [[1.0000e+00, 3.5176e-05]],\n",
       " \n",
       "         [[1.0000e+00, 2.0791e-04]],\n",
       " \n",
       "         [[1.0000e+00, 2.0536e-04]],\n",
       " \n",
       "         [[1.0000e+00, 2.5434e-04]],\n",
       " \n",
       "         [[1.0000e+00, 1.9544e-04]],\n",
       " \n",
       "         [[1.0000e+00, 6.2095e-04]],\n",
       " \n",
       "         [[1.0000e+00, 1.1509e-03]],\n",
       " \n",
       "         [[1.0000e+00, 1.5344e-03]],\n",
       " \n",
       "         [[1.0000e+00, 8.8020e-04]],\n",
       " \n",
       "         [[1.0000e+00, 6.1326e-05]],\n",
       " \n",
       "         [[1.0000e+00, 1.9962e-05]],\n",
       " \n",
       "         [[1.0000e+00, 4.2448e-05]],\n",
       " \n",
       "         [[1.0000e+00, 6.0079e-04]],\n",
       " \n",
       "         [[1.0000e+00, 5.5214e-05]],\n",
       " \n",
       "         [[1.0000e+00, 1.9901e-04]],\n",
       " \n",
       "         [[1.0000e+00, 8.5075e-05]],\n",
       " \n",
       "         [[1.0000e+00, 1.2409e-04]],\n",
       " \n",
       "         [[1.0000e+00, 1.7719e-04]],\n",
       " \n",
       "         [[1.0000e+00, 2.2641e-04]],\n",
       " \n",
       "         [[1.0000e+00, 3.7015e-04]],\n",
       " \n",
       "         [[1.0000e+00, 1.1712e-04]],\n",
       " \n",
       "         [[1.0000e+00, 1.8079e-04]],\n",
       " \n",
       "         [[1.0000e+00, 9.7392e-05]],\n",
       " \n",
       "         [[1.0000e+00, 1.0855e-04]],\n",
       " \n",
       "         [[1.0000e+00, 3.5755e-05]],\n",
       " \n",
       "         [[1.0000e+00, 2.0259e-04]],\n",
       " \n",
       "         [[1.0000e+00, 2.2480e-04]],\n",
       " \n",
       "         [[1.0000e+00, 2.5953e-04]],\n",
       " \n",
       "         [[1.0000e+00, 1.9171e-04]],\n",
       " \n",
       "         [[1.0000e+00, 6.1192e-04]],\n",
       " \n",
       "         [[1.0000e+00, 1.1617e-03]],\n",
       " \n",
       "         [[1.0000e+00, 1.4832e-03]],\n",
       " \n",
       "         [[1.0000e+00, 9.1477e-04]],\n",
       " \n",
       "         [[1.0000e+00, 6.1343e-05]],\n",
       " \n",
       "         [[1.0000e+00, 1.9889e-05]],\n",
       " \n",
       "         [[1.0000e+00, 4.1314e-05]],\n",
       " \n",
       "         [[1.0000e+00, 5.9129e-04]],\n",
       " \n",
       "         [[1.0000e+00, 5.4197e-05]],\n",
       " \n",
       "         [[1.0000e+00, 1.9917e-04]],\n",
       " \n",
       "         [[1.0000e+00, 8.6183e-05]],\n",
       " \n",
       "         [[1.0000e+00, 1.2329e-04]],\n",
       " \n",
       "         [[1.0000e+00, 1.7851e-04]],\n",
       " \n",
       "         [[1.0000e+00, 2.2645e-04]],\n",
       " \n",
       "         [[1.0000e+00, 3.6791e-04]],\n",
       " \n",
       "         [[1.0000e+00, 1.1692e-04]],\n",
       " \n",
       "         [[1.0000e+00, 1.8111e-04]],\n",
       " \n",
       "         [[1.0000e+00, 4.8639e-05]],\n",
       " \n",
       "         [[1.0000e+00, 1.1123e-04]],\n",
       " \n",
       "         [[1.0000e+00, 3.6206e-05]],\n",
       " \n",
       "         [[1.0000e+00, 1.9859e-04]],\n",
       " \n",
       "         [[1.0000e+00, 2.3826e-04]],\n",
       " \n",
       "         [[1.0000e+00, 2.6478e-04]],\n",
       " \n",
       "         [[1.0000e+00, 1.8421e-04]],\n",
       " \n",
       "         [[1.0000e+00, 6.0893e-04]],\n",
       " \n",
       "         [[1.0000e+00, 1.1445e-03]],\n",
       " \n",
       "         [[1.0000e+00, 1.4554e-03]],\n",
       " \n",
       "         [[1.0000e+00, 9.1579e-04]],\n",
       " \n",
       "         [[1.0000e+00, 6.1328e-05]],\n",
       " \n",
       "         [[1.0000e+00, 2.9811e-05]],\n",
       " \n",
       "         [[1.0000e+00, 4.0458e-05]],\n",
       " \n",
       "         [[1.0000e+00, 5.9378e-04]],\n",
       " \n",
       "         [[1.0000e+00, 5.3553e-05]],\n",
       " \n",
       "         [[1.0000e+00, 1.9932e-04]],\n",
       " \n",
       "         [[1.0000e+00, 8.6479e-05]],\n",
       " \n",
       "         [[1.0000e+00, 1.2116e-04]],\n",
       " \n",
       "         [[1.0000e+00, 1.7896e-04]],\n",
       " \n",
       "         [[1.0000e+00, 2.2722e-04]],\n",
       " \n",
       "         [[1.0000e+00, 3.6583e-04]],\n",
       " \n",
       "         [[1.0000e+00, 1.1648e-04]],\n",
       " \n",
       "         [[1.0000e+00, 1.7997e-04]],\n",
       " \n",
       "         [[1.0000e+00, 4.8784e-05]],\n",
       " \n",
       "         [[1.0000e+00, 1.1621e-04]],\n",
       " \n",
       "         [[1.0000e+00, 3.6306e-05]],\n",
       " \n",
       "         [[1.0000e+00, 1.9421e-04]],\n",
       " \n",
       "         [[1.0000e+00, 2.4756e-04]],\n",
       " \n",
       "         [[1.0000e+00, 2.7109e-04]],\n",
       " \n",
       "         [[1.0000e+00, 1.7984e-04]],\n",
       " \n",
       "         [[1.0000e+00, 6.1121e-04]],\n",
       " \n",
       "         [[1.0000e+00, 1.1566e-03]],\n",
       " \n",
       "         [[1.0000e+00, 1.4071e-03]],\n",
       " \n",
       "         [[1.0000e+00, 9.3010e-04]],\n",
       " \n",
       "         [[1.0000e+00, 6.0372e-05]],\n",
       " \n",
       "         [[1.0000e+00, 3.0680e-05]],\n",
       " \n",
       "         [[1.0000e+00, 3.9675e-05]],\n",
       " \n",
       "         [[1.0000e+00, 5.8301e-04]],\n",
       " \n",
       "         [[1.0000e+00, 5.3159e-05]],\n",
       " \n",
       "         [[1.0000e+00, 1.9947e-04]],\n",
       " \n",
       "         [[1.0000e+00, 8.6936e-05]],\n",
       " \n",
       "         [[1.0000e+00, 1.1946e-04]],\n",
       " \n",
       "         [[1.0000e+00, 1.7846e-04]],\n",
       " \n",
       "         [[1.0000e+00, 2.2716e-04]],\n",
       " \n",
       "         [[1.0000e+00, 3.6573e-04]],\n",
       " \n",
       "         [[1.0000e+00, 1.1591e-04]],\n",
       " \n",
       "         [[1.0000e+00, 1.7917e-04]],\n",
       " \n",
       "         [[1.0000e+00, 4.8542e-05]],\n",
       " \n",
       "         [[1.0000e+00, 1.1900e-04]],\n",
       " \n",
       "         [[1.0000e+00, 3.6401e-05]],\n",
       " \n",
       "         [[1.0000e+00, 1.9130e-04]],\n",
       " \n",
       "         [[1.0000e+00, 2.6030e-04]],\n",
       " \n",
       "         [[1.0000e+00, 2.7740e-04]],\n",
       " \n",
       "         [[1.0000e+00, 1.7532e-04]],\n",
       " \n",
       "         [[1.0000e+00, 6.0668e-04]],\n",
       " \n",
       "         [[1.0000e+00, 1.1430e-03]],\n",
       " \n",
       "         [[1.0000e+00, 1.3951e-03]],\n",
       " \n",
       "         [[1.0000e+00, 9.4797e-04]],\n",
       " \n",
       "         [[1.0000e+00, 6.0556e-05]],\n",
       " \n",
       "         [[1.0000e+00, 3.1901e-05]],\n",
       " \n",
       "         [[1.0000e+00, 3.8276e-05]],\n",
       " \n",
       "         [[1.0000e+00, 5.8656e-04]],\n",
       " \n",
       "         [[1.0000e+00, 5.2128e-05]],\n",
       " \n",
       "         [[1.0000e+00, 1.9962e-04]],\n",
       " \n",
       "         [[1.0000e+00, 8.8087e-05]],\n",
       " \n",
       "         [[1.0000e+00, 1.1752e-04]],\n",
       " \n",
       "         [[1.0000e+00, 1.7841e-04]],\n",
       " \n",
       "         [[1.0000e+00, 2.2674e-04]],\n",
       " \n",
       "         [[1.0000e+00, 3.6351e-04]],\n",
       " \n",
       "         [[1.0000e+00, 1.1538e-04]],\n",
       " \n",
       "         [[1.0000e+00, 1.7854e-04]],\n",
       " \n",
       "         [[1.0000e+00, 4.8803e-05]],\n",
       " \n",
       "         [[1.0000e+00, 1.2233e-04]],\n",
       " \n",
       "         [[1.0000e+00, 3.7095e-05]],\n",
       " \n",
       "         [[1.0000e+00, 1.6635e-04]],\n",
       " \n",
       "         [[1.0000e+00, 2.6908e-04]],\n",
       " \n",
       "         [[1.0000e+00, 2.8280e-04]],\n",
       " \n",
       "         [[1.0000e+00, 1.6895e-04]],\n",
       " \n",
       "         [[1.0000e+00, 6.0042e-04]],\n",
       " \n",
       "         [[1.0000e+00, 1.1360e-03]],\n",
       " \n",
       "         [[1.0000e+00, 1.3570e-03]],\n",
       " \n",
       "         [[1.0000e+00, 9.5489e-04]],\n",
       " \n",
       "         [[1.0000e+00, 6.0588e-05]],\n",
       " \n",
       "         [[1.0000e+00, 3.2612e-05]],\n",
       " \n",
       "         [[1.0000e+00, 3.7308e-05]],\n",
       " \n",
       "         [[1.0000e+00, 5.7696e-04]],\n",
       " \n",
       "         [[1.0000e+00, 5.1839e-05]],\n",
       " \n",
       "         [[1.0000e+00, 1.9978e-04]],\n",
       " \n",
       "         [[1.0000e+00, 8.9254e-05]],\n",
       " \n",
       "         [[1.0000e+00, 1.1549e-04]],\n",
       " \n",
       "         [[1.0000e+00, 1.7889e-04]],\n",
       " \n",
       "         [[1.0000e+00, 2.2659e-04]],\n",
       " \n",
       "         [[1.0000e+00, 3.6084e-04]],\n",
       " \n",
       "         [[1.0000e+00, 1.1510e-04]],\n",
       " \n",
       "         [[1.0000e+00, 1.7624e-04]],\n",
       " \n",
       "         [[1.0000e+00, 4.9250e-05]],\n",
       " \n",
       "         [[1.0000e+00, 1.2442e-04]],\n",
       " \n",
       "         [[1.0000e+00, 3.7669e-05]],\n",
       " \n",
       "         [[1.0000e+00, 1.6494e-04]],\n",
       " \n",
       "         [[1.0000e+00, 2.7638e-04]],\n",
       " \n",
       "         [[1.0000e+00, 2.9092e-04]],\n",
       " \n",
       "         [[1.0000e+00, 3.7270e-04]],\n",
       " \n",
       "         [[1.0000e+00, 6.0167e-04]],\n",
       " \n",
       "         [[1.0000e+00, 1.1108e-03]],\n",
       " \n",
       "         [[1.0000e+00, 1.3285e-03]],\n",
       " \n",
       "         [[1.0000e+00, 9.6887e-04]],\n",
       " \n",
       "         [[1.0000e+00, 6.0254e-05]],\n",
       " \n",
       "         [[1.0000e+00, 3.4051e-05]],\n",
       " \n",
       "         [[1.0000e+00, 3.7013e-05]],\n",
       " \n",
       "         [[1.0000e+00, 5.7552e-04]],\n",
       " \n",
       "         [[1.0000e+00, 5.1227e-05]],\n",
       " \n",
       "         [[1.0000e+00, 1.9994e-04]],\n",
       " \n",
       "         [[1.0000e+00, 8.9634e-05]],\n",
       " \n",
       "         [[1.0000e+00, 1.1271e-04]],\n",
       " \n",
       "         [[1.0000e+00, 1.7866e-04]],\n",
       " \n",
       "         [[1.0000e+00, 2.2695e-04]],\n",
       " \n",
       "         [[1.0000e+00, 3.5851e-04]],\n",
       " \n",
       "         [[1.0000e+00, 1.1463e-04]],\n",
       " \n",
       "         [[1.0000e+00, 1.7576e-04]],\n",
       " \n",
       "         [[1.0000e+00, 4.9289e-05]],\n",
       " \n",
       "         [[1.0000e+00, 1.2612e-04]],\n",
       " \n",
       "         [[1.0000e+00, 3.7620e-05]],\n",
       " \n",
       "         [[1.0000e+00, 1.6439e-04]],\n",
       " \n",
       "         [[1.0000e+00, 2.8313e-04]],\n",
       " \n",
       "         [[1.0000e+00, 2.9855e-04]],\n",
       " \n",
       "         [[1.0000e+00, 3.6741e-04]],\n",
       " \n",
       "         [[1.0000e+00, 5.9862e-04]],\n",
       " \n",
       "         [[1.0000e+00, 1.1128e-03]],\n",
       " \n",
       "         [[1.0000e+00, 1.3146e-03]],\n",
       " \n",
       "         [[1.0000e+00, 1.0054e-03]],\n",
       " \n",
       "         [[1.0000e+00, 5.9979e-05]],\n",
       " \n",
       "         [[1.0000e+00, 3.5073e-05]],\n",
       " \n",
       "         [[1.0000e+00, 3.5863e-05]],\n",
       " \n",
       "         [[1.0000e+00, 5.7116e-04]],\n",
       " \n",
       "         [[1.0000e+00, 5.0266e-05]],\n",
       " \n",
       "         [[1.0000e+00, 2.0010e-04]],\n",
       " \n",
       "         [[1.0000e+00, 9.0150e-05]],\n",
       " \n",
       "         [[1.0000e+00, 1.1122e-04]]]),\n",
       " tensor([[0.7681],\n",
       "         [1.2538],\n",
       "         [1.5649],\n",
       "         [1.6769],\n",
       "         [1.5942],\n",
       "         [1.3466],\n",
       "         [0.9175],\n",
       "         [0.3394],\n",
       "         [0.0285],\n",
       "         [0.0307],\n",
       "         [0.0308],\n",
       "         [0.0289],\n",
       "         [0.0252],\n",
       "         [0.0200],\n",
       "         [0.0136],\n",
       "         [0.0065],\n",
       "         [0.0105],\n",
       "         [0.0471],\n",
       "         [0.0810],\n",
       "         [0.1101],\n",
       "         [0.1325],\n",
       "         [0.1465],\n",
       "         [0.1510],\n",
       "         [0.1452],\n",
       "         [0.7576],\n",
       "         [1.2457],\n",
       "         [1.5597],\n",
       "         [1.6748],\n",
       "         [1.5945],\n",
       "         [1.3499],\n",
       "         [0.9233],\n",
       "         [0.3472],\n",
       "         [0.0285],\n",
       "         [0.0307],\n",
       "         [0.0308],\n",
       "         [0.0289],\n",
       "         [0.0253],\n",
       "         [0.0201],\n",
       "         [0.0137],\n",
       "         [0.0066],\n",
       "         [0.0099],\n",
       "         [0.0466],\n",
       "         [0.0805],\n",
       "         [0.1097],\n",
       "         [0.1322],\n",
       "         [0.1464],\n",
       "         [0.1510],\n",
       "         [0.1454],\n",
       "         [0.7471],\n",
       "         [1.2376],\n",
       "         [1.5544],\n",
       "         [1.6727],\n",
       "         [1.5948],\n",
       "         [1.3531],\n",
       "         [0.9291],\n",
       "         [0.3550],\n",
       "         [0.0284],\n",
       "         [0.0307],\n",
       "         [0.0308],\n",
       "         [0.0290],\n",
       "         [0.0253],\n",
       "         [0.0202],\n",
       "         [0.0138],\n",
       "         [0.0067],\n",
       "         [0.0094],\n",
       "         [0.0461],\n",
       "         [0.0801],\n",
       "         [0.1093],\n",
       "         [0.1319],\n",
       "         [0.1462],\n",
       "         [0.1510],\n",
       "         [0.1455],\n",
       "         [0.7366],\n",
       "         [1.2295],\n",
       "         [1.5492],\n",
       "         [1.6706],\n",
       "         [1.5951],\n",
       "         [1.3563],\n",
       "         [0.9347],\n",
       "         [0.3626],\n",
       "         [0.0284],\n",
       "         [0.0306],\n",
       "         [0.0308],\n",
       "         [0.0290],\n",
       "         [0.0254],\n",
       "         [0.0203],\n",
       "         [0.0139],\n",
       "         [0.0068],\n",
       "         [0.0088],\n",
       "         [0.0455],\n",
       "         [0.0796],\n",
       "         [0.1089],\n",
       "         [0.1316],\n",
       "         [0.1461],\n",
       "         [0.1510],\n",
       "         [0.1457],\n",
       "         [0.7264],\n",
       "         [1.2216],\n",
       "         [1.5441],\n",
       "         [1.6685],\n",
       "         [1.5953],\n",
       "         [1.3595],\n",
       "         [0.9404],\n",
       "         [0.3702],\n",
       "         [0.0283],\n",
       "         [0.0306],\n",
       "         [0.0308],\n",
       "         [0.0291],\n",
       "         [0.0255],\n",
       "         [0.0203],\n",
       "         [0.0140],\n",
       "         [0.0070],\n",
       "         [0.0083],\n",
       "         [0.0450],\n",
       "         [0.0791],\n",
       "         [0.1085],\n",
       "         [0.1314],\n",
       "         [0.1459],\n",
       "         [0.1510],\n",
       "         [0.1458],\n",
       "         [0.7162],\n",
       "         [1.2137],\n",
       "         [1.5389],\n",
       "         [1.6664],\n",
       "         [1.5955],\n",
       "         [1.3625],\n",
       "         [0.9459],\n",
       "         [0.3776],\n",
       "         [0.0283],\n",
       "         [0.0306],\n",
       "         [0.0309],\n",
       "         [0.0291],\n",
       "         [0.0255],\n",
       "         [0.0204],\n",
       "         [0.0141],\n",
       "         [0.0071],\n",
       "         [0.0077],\n",
       "         [0.0445],\n",
       "         [0.0786],\n",
       "         [0.1082],\n",
       "         [0.1311],\n",
       "         [0.1458],\n",
       "         [0.1510],\n",
       "         [0.1460],\n",
       "         [0.7060],\n",
       "         [1.2058],\n",
       "         [1.5337],\n",
       "         [1.6642],\n",
       "         [1.5957],\n",
       "         [1.3656],\n",
       "         [0.9514],\n",
       "         [0.3851],\n",
       "         [0.0282],\n",
       "         [0.0306],\n",
       "         [0.0309],\n",
       "         [0.0291],\n",
       "         [0.0256],\n",
       "         [0.0205],\n",
       "         [0.0142],\n",
       "         [0.0072],\n",
       "         [0.0072],\n",
       "         [0.0439],\n",
       "         [0.0782],\n",
       "         [0.1078],\n",
       "         [0.1308],\n",
       "         [0.1456],\n",
       "         [0.1510],\n",
       "         [0.1461],\n",
       "         [0.6959],\n",
       "         [1.1979],\n",
       "         [1.5286],\n",
       "         [1.6621],\n",
       "         [1.5959],\n",
       "         [1.3686],\n",
       "         [0.9570],\n",
       "         [0.3925],\n",
       "         [0.0282],\n",
       "         [0.0306],\n",
       "         [0.0309],\n",
       "         [0.0292],\n",
       "         [0.0257],\n",
       "         [0.0206],\n",
       "         [0.0143],\n",
       "         [0.0073],\n",
       "         [0.0066],\n",
       "         [0.0434],\n",
       "         [0.0777],\n",
       "         [0.1074],\n",
       "         [0.1305],\n",
       "         [0.1455],\n",
       "         [0.1510],\n",
       "         [0.1463],\n",
       "         [0.6858],\n",
       "         [1.1900],\n",
       "         [1.5235],\n",
       "         [1.6599],\n",
       "         [1.5960],\n",
       "         [1.3716],\n",
       "         [0.9624],\n",
       "         [0.3999],\n",
       "         [0.0281],\n",
       "         [0.0305],\n",
       "         [0.0309],\n",
       "         [0.0292],\n",
       "         [0.0257],\n",
       "         [0.0207],\n",
       "         [0.0144],\n",
       "         [0.0074],\n",
       "         [0.0061],\n",
       "         [0.0429],\n",
       "         [0.0772],\n",
       "         [0.1070],\n",
       "         [0.1303],\n",
       "         [0.1453],\n",
       "         [0.1510],\n",
       "         [0.1464],\n",
       "         [0.6758],\n",
       "         [1.1822],\n",
       "         [1.5183],\n",
       "         [1.6578],\n",
       "         [1.5963],\n",
       "         [1.3748],\n",
       "         [0.9681],\n",
       "         [0.4076],\n",
       "         [0.0281],\n",
       "         [0.0305],\n",
       "         [0.0309],\n",
       "         [0.0293],\n",
       "         [0.0258],\n",
       "         [0.0208],\n",
       "         [0.0145],\n",
       "         [0.0075],\n",
       "         [0.0055],\n",
       "         [0.0424],\n",
       "         [0.0768],\n",
       "         [0.1066],\n",
       "         [0.1300],\n",
       "         [0.1452],\n",
       "         [0.1510],\n",
       "         [0.1466],\n",
       "         [0.6657],\n",
       "         [1.1744],\n",
       "         [1.5132],\n",
       "         [1.6558],\n",
       "         [1.5965],\n",
       "         [1.3779],\n",
       "         [0.9739],\n",
       "         [0.4154],\n",
       "         [0.0280],\n",
       "         [0.0305],\n",
       "         [0.0309],\n",
       "         [0.0293],\n",
       "         [0.0258],\n",
       "         [0.0208],\n",
       "         [0.0146],\n",
       "         [0.0076]]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "import pygame as pg \n",
    "from robots import CarLikeBot, CircleRobot\n",
    "from random import uniform\n",
    "from env import Environment\n",
    "from policies.policies import MultiRobotNavigation\n",
    "from algo.ppo import sample_trajectories\n",
    "from rewards import RewardCircle\n",
    "import torch\n",
    "\n",
    "env = Environment(CircleRobot, 3, reward=RewardCircle())\n",
    "h, w = env.shape\n",
    "env.get_observation()\n",
    "\n",
    "p = MultiRobotNavigation(env.agents[0].history)\n",
    "\n",
    "T =256\n",
    "\n",
    "\n",
    "# print(actions)\n",
    "# print(rewards)\n",
    "sample_trajectories(env, p, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kajratbekbolinov/anaconda3/lib/python3.11/site-packages/keras/src/layers/layer.py:357: UserWarning: `build()` was called on layer 'actor_critic_17', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/Users/kajratbekbolinov/anaconda3/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states.hsape (18, 4)\n",
      "states.hsape (23, 4)\n",
      "states.hsape (16, 4)\n",
      "states.hsape (18, 4)\n",
      "states.hsape (13, 4)\n",
      "states.hsape (13, 4)\n",
      "states.hsape (18, 4)\n",
      "states.hsape (21, 4)\n",
      "states.hsape (10, 4)\n",
      "states.hsape (23, 4)\n",
      "states.hsape (40, 4)\n",
      "states.hsape (22, 4)\n",
      "states.hsape (33, 4)\n",
      "states.hsape (52, 4)\n",
      "states.hsape (46, 4)\n",
      "states.hsape (55, 4)\n",
      "states.hsape (14, 4)\n",
      "states.hsape (30, 4)\n",
      "states.hsape (94, 4)\n",
      "states.hsape (65, 4)\n",
      "states.hsape (78, 4)\n",
      "states.hsape (29, 4)\n",
      "states.hsape (66, 4)\n",
      "states.hsape (62, 4)\n",
      "states.hsape (62, 4)\n",
      "states.hsape (66, 4)\n",
      "states.hsape (127, 4)\n",
      "states.hsape (72, 4)\n",
      "states.hsape (104, 4)\n",
      "states.hsape (55, 4)\n",
      "states.hsape (52, 4)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 115\u001b[0m\n\u001b[1;32m    112\u001b[0m returns_batch \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(returns_batch)\n\u001b[1;32m    113\u001b[0m old_logits, _ \u001b[38;5;241m=\u001b[39m model(states)\n\u001b[0;32m--> 115\u001b[0m loss \u001b[38;5;241m=\u001b[39m ppo_loss(old_logits, values, returns_batch \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39marray(values), states, actions, returns_batch)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# print(f\"Episode: {episode + 1}, Loss: {loss.numpy()}\")\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[24], line 67\u001b[0m, in \u001b[0;36mppo_loss\u001b[0;34m(old_logits, old_values, advantages, states, actions, returns)\u001b[0m\n\u001b[1;32m     65\u001b[0m advantages \u001b[38;5;241m=\u001b[39m get_advantages(returns, old_values)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 67\u001b[0m     loss \u001b[38;5;241m=\u001b[39m train_step(states, actions, returns, old_logits, old_values)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "Cell \u001b[0;32mIn[24], line 62\u001b[0m, in \u001b[0;36mppo_loss.<locals>.train_step\u001b[0;34m(states, actions, returns, old_logits, old_values)\u001b[0m\n\u001b[1;32m     60\u001b[0m     loss \u001b[38;5;241m=\u001b[39m compute_loss(logits, values, actions, returns)\n\u001b[1;32m     61\u001b[0m gradients \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, model\u001b[38;5;241m.\u001b[39mtrainable_variables)\n\u001b[0;32m---> 62\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(gradients, model\u001b[38;5;241m.\u001b[39mtrainable_variables))\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/optimizers/base_optimizer.py:269\u001b[0m, in \u001b[0;36mBaseOptimizer.apply_gradients\u001b[0;34m(self, grads_and_vars)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_gradients\u001b[39m(\u001b[38;5;28mself\u001b[39m, grads_and_vars):\n\u001b[1;32m    268\u001b[0m     grads, trainable_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mgrads_and_vars)\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply(grads, trainable_variables)\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;66;03m# Return iterations for compat with tf.keras.\u001b[39;00m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterations\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/optimizers/base_optimizer.py:330\u001b[0m, in \u001b[0;36mBaseOptimizer.apply\u001b[0;34m(self, grads, trainable_variables)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_weight_decay(trainable_variables)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# Apply gradient updates.\u001b[39;00m\n\u001b[0;32m--> 330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend_apply_gradients(grads, trainable_variables)\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# Apply variable constraints after applying gradients.\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m variable \u001b[38;5;129;01min\u001b[39;00m trainable_variables:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/optimizers/base_optimizer.py:380\u001b[0m, in \u001b[0;36mBaseOptimizer._backend_apply_gradients\u001b[0;34m(self, grads, trainable_variables)\u001b[0m\n\u001b[1;32m    373\u001b[0m     ops\u001b[38;5;241m.\u001b[39mcond(\n\u001b[1;32m    374\u001b[0m         is_update_step,\n\u001b[1;32m    375\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: _update_step_fn(\u001b[38;5;28mself\u001b[39m, grads, trainable_variables),\n\u001b[1;32m    376\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: _grad_accumulation_fn(\u001b[38;5;28mself\u001b[39m, grads),\n\u001b[1;32m    377\u001b[0m     )\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;66;03m# Run udpate step.\u001b[39;00m\n\u001b[0;32m--> 380\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend_update_step(\n\u001b[1;32m    381\u001b[0m         grads, trainable_variables, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate\n\u001b[1;32m    382\u001b[0m     )\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_ema:\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_variables_moving_average(\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainable_variables\n\u001b[1;32m    387\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/backend/tensorflow/optimizer.py:117\u001b[0m, in \u001b[0;36mTFOptimizer._backend_update_step\u001b[0;34m(self, grads, trainable_variables, learning_rate)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_backend_update_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, grads, trainable_variables, learning_rate):\n\u001b[1;32m    113\u001b[0m     trainable_variables \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    114\u001b[0m         v\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, backend\u001b[38;5;241m.\u001b[39mVariable) \u001b[38;5;28;01melse\u001b[39;00m v\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m trainable_variables\n\u001b[1;32m    116\u001b[0m     ]\n\u001b[0;32m--> 117\u001b[0m     tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39minterim\u001b[38;5;241m.\u001b[39mmaybe_merge_call(\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distributed_tf_update_step,\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribution_strategy,\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(grads, trainable_variables)),\n\u001b[1;32m    121\u001b[0m         learning_rate,\n\u001b[1;32m    122\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/distribute/merge_call_interim.py:51\u001b[0m, in \u001b[0;36mmaybe_merge_call\u001b[0;34m(fn, strategy, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Maybe invoke `fn` via `merge_call` which may or may not be fulfilled.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03mThe caller of this utility function requests to invoke `fn` via `merge_call`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03m  The return value of the `fn` call.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m strategy_supports_no_merge_call():\n\u001b[0;32m---> 51\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(strategy, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m distribute_lib\u001b[38;5;241m.\u001b[39mget_replica_context()\u001b[38;5;241m.\u001b[39mmerge_call(\n\u001b[1;32m     54\u001b[0m       fn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/backend/tensorflow/optimizer.py:131\u001b[0m, in \u001b[0;36mTFOptimizer._distributed_tf_update_step\u001b[0;34m(self, distribution, grads_and_vars, learning_rate)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_step(grad, var, learning_rate)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m grad, var \u001b[38;5;129;01min\u001b[39;00m grads_and_vars:\n\u001b[0;32m--> 131\u001b[0m     distribution\u001b[38;5;241m.\u001b[39mextended\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m    132\u001b[0m         var, apply_grad_to_update_var, args\u001b[38;5;241m=\u001b[39m(grad,), group\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    133\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/distribute/distribute_lib.py:3005\u001b[0m, in \u001b[0;36mStrategyExtendedV2.update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   3002\u001b[0m   fn \u001b[38;5;241m=\u001b[39m autograph\u001b[38;5;241m.\u001b[39mtf_convert(\n\u001b[1;32m   3003\u001b[0m       fn, autograph_ctx\u001b[38;5;241m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   3004\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container_strategy()\u001b[38;5;241m.\u001b[39mscope():\n\u001b[0;32m-> 3005\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update(var, fn, args, kwargs, group)\n\u001b[1;32m   3006\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3007\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_replica_ctx_update(\n\u001b[1;32m   3008\u001b[0m       var, fn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs, group\u001b[38;5;241m=\u001b[39mgroup)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/distribute/distribute_lib.py:4075\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   4072\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update\u001b[39m(\u001b[38;5;28mself\u001b[39m, var, fn, args, kwargs, group):\n\u001b[1;32m   4073\u001b[0m   \u001b[38;5;66;03m# The implementations of _update() and _update_non_slot() are identical\u001b[39;00m\n\u001b[1;32m   4074\u001b[0m   \u001b[38;5;66;03m# except _update() passes `var` as the first argument to `fn()`.\u001b[39;00m\n\u001b[0;32m-> 4075\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_non_slot(var, fn, (var,) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mtuple\u001b[39m(args), kwargs, group)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/distribute/distribute_lib.py:4081\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update_non_slot\u001b[0;34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[0m\n\u001b[1;32m   4077\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_non_slot\u001b[39m(\u001b[38;5;28mself\u001b[39m, colocate_with, fn, args, kwargs, should_group):\n\u001b[1;32m   4078\u001b[0m   \u001b[38;5;66;03m# TODO(josh11b): Figure out what we should be passing to UpdateContext()\u001b[39;00m\n\u001b[1;32m   4079\u001b[0m   \u001b[38;5;66;03m# once that value is used for something.\u001b[39;00m\n\u001b[1;32m   4080\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m UpdateContext(colocate_with):\n\u001b[0;32m-> 4081\u001b[0m     result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   4082\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_group:\n\u001b[1;32m   4083\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py:596\u001b[0m, in \u001b[0;36mcall_with_unspecified_conversion_status.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    595\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx\u001b[38;5;241m.\u001b[39mControlStatusCtx(status\u001b[38;5;241m=\u001b[39mag_ctx\u001b[38;5;241m.\u001b[39mStatus\u001b[38;5;241m.\u001b[39mUNSPECIFIED):\n\u001b[0;32m--> 596\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/backend/tensorflow/optimizer.py:128\u001b[0m, in \u001b[0;36mTFOptimizer._distributed_tf_update_step.<locals>.apply_grad_to_update_var\u001b[0;34m(var, grad)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_grad_to_update_var\u001b[39m(var, grad):\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_step(grad, var, learning_rate)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/optimizers/adam.py:126\u001b[0m, in \u001b[0;36mAdam.update_step\u001b[0;34m(self, gradient, variable, learning_rate)\u001b[0m\n\u001b[1;32m    123\u001b[0m m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_momentums[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_variable_index(variable)]\n\u001b[1;32m    124\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_velocities[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_variable_index(variable)]\n\u001b[0;32m--> 126\u001b[0m alpha \u001b[38;5;241m=\u001b[39m lr \u001b[38;5;241m*\u001b[39m ops\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta_2_power) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta_1_power)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massign_add(\n\u001b[1;32m    129\u001b[0m     m, ops\u001b[38;5;241m.\u001b[39mmultiply(ops\u001b[38;5;241m.\u001b[39msubtract(gradient, m), \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta_1)\n\u001b[1;32m    130\u001b[0m )\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massign_add(\n\u001b[1;32m    132\u001b[0m     v,\n\u001b[1;32m    133\u001b[0m     ops\u001b[38;5;241m.\u001b[39mmultiply(\n\u001b[1;32m    134\u001b[0m         ops\u001b[38;5;241m.\u001b[39msubtract(ops\u001b[38;5;241m.\u001b[39msquare(gradient), v), \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta_2\n\u001b[1;32m    135\u001b[0m     ),\n\u001b[1;32m    136\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/framework/override_binary_operator.py:150\u001b[0m, in \u001b[0;36moverride_binary_operator_helper.<locals>.r_binary_op_wrapper\u001b[0;34m(y, x)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mname_scope(\u001b[38;5;28;01mNone\u001b[39;00m, op_name, [x, y]) \u001b[38;5;28;01mas\u001b[39;00m name:\n\u001b[1;32m    147\u001b[0m   \u001b[38;5;66;03m# TODO(b/178860388): Figure out why binary_op_wrapper and\u001b[39;00m\n\u001b[1;32m    148\u001b[0m   \u001b[38;5;66;03m#   r_binary_op_wrapper use different force_same_dtype values.\u001b[39;00m\n\u001b[1;32m    149\u001b[0m   y, x \u001b[38;5;241m=\u001b[39m maybe_promote_tensors(y, x, force_same_dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m func(x, y, name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/ops/tensor_math_operator_overrides.py:82\u001b[0m, in \u001b[0;36m_subtract_factory\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_subtract_factory\u001b[39m(x, y, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     80\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m math_ops\n\u001b[0;32m---> 82\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m math_ops\u001b[38;5;241m.\u001b[39msubtract(x, y, name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Environment setup\n",
    "env = gym.make('CartPole-v1')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Hyperparameters\n",
    "gamma = 0.99  # Discount factor\n",
    "lr_actor = 0.001  # Actor learning rate\n",
    "lr_critic = 0.001  # Critic learning rate\n",
    "clip_ratio = 0.2  # PPO clip ratio\n",
    "epochs = 10  # Number of optimization epochs\n",
    "batch_size = 64  # Batch size for optimization\n",
    "\n",
    "# Actor and Critic networks\n",
    "class ActorCritic(tf.keras.Model):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(64, activation='relu')\n",
    "        self.policy_logits = tf.keras.layers.Dense(action_size)\n",
    "        self.dense2 = tf.keras.layers.Dense(64, activation='relu')\n",
    "        self.value = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, state):\n",
    "        x = self.dense1(state)\n",
    "        logits = self.policy_logits(x)\n",
    "        value = self.value(x)\n",
    "        return logits, value\n",
    "\n",
    "# PPO algorithm\n",
    "def ppo_loss(old_logits, old_values, advantages, states, actions, returns):\n",
    "    print(\"states.hsape\",states.shape)\n",
    "    def compute_loss(logits, values, actions, returns):\n",
    "        actions_onehot = tf.one_hot(actions, action_size, dtype=tf.float32)\n",
    "        policy = tf.nn.softmax(logits)\n",
    "        action_probs = tf.reduce_sum(actions_onehot * policy, axis=1)\n",
    "        old_policy = tf.nn.softmax(old_logits)\n",
    "        old_action_probs = tf.reduce_sum(actions_onehot * old_policy, axis=1)\n",
    "\n",
    "        # Policy loss\n",
    "        ratio = tf.exp(tf.math.log(action_probs + 1e-10) - tf.math.log(old_action_probs + 1e-10))\n",
    "        clipped_ratio = tf.clip_by_value(ratio, 1 - clip_ratio, 1 + clip_ratio)\n",
    "        policy_loss = -tf.reduce_mean(tf.minimum(ratio * advantages, clipped_ratio * advantages))\n",
    "\n",
    "        # Value loss\n",
    "        value_loss = tf.reduce_mean(tf.square(values - returns))\n",
    "\n",
    "        # Entropy bonus (optional)\n",
    "        entropy_bonus = tf.reduce_mean(policy * tf.math.log(policy + 1e-10))\n",
    "\n",
    "        total_loss = policy_loss + 0.5 * value_loss - 0.01 * entropy_bonus  # Entropy regularization\n",
    "        return total_loss\n",
    "\n",
    "    def get_advantages(returns, values):\n",
    "        advantages = returns - values\n",
    "        return (advantages - tf.reduce_mean(advantages)) / (tf.math.reduce_std(advantages) + 1e-8)\n",
    "\n",
    "    def train_step(states, actions, returns, old_logits, old_values):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits, values = model(states)\n",
    "            loss = compute_loss(logits, values, actions, returns)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "    advantages = get_advantages(returns, old_values)\n",
    "    for _ in range(epochs):\n",
    "        loss = train_step(states, actions, returns, old_logits, old_values)\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Initialize actor-critic model and optimizer\n",
    "model = ActorCritic(state_size, action_size)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_actor)\n",
    "\n",
    "# Main training loop\n",
    "max_episodes = 1000\n",
    "max_steps_per_episode = 512\n",
    "for episode in range(max_episodes):\n",
    "    states, actions, rewards, values, returns = [], [], [], [], []\n",
    "    state = env.reset()[0]\n",
    "    for step in range(max_steps_per_episode):\n",
    "        # print(state)\n",
    "        state = tf.expand_dims(tf.convert_to_tensor(state, dtype=tf.float32), 0)\n",
    "        logits, value = model(state)\n",
    "\n",
    "        # Sample action from the policy distribution\n",
    "        # print(\"logits\",logits)\n",
    "        action = tf.random.categorical(logits, 1)[0, 0].numpy()\n",
    "        # print(\"action\", action)\n",
    "        # print(len(env.step(action)))\n",
    "        # print(env.step(action))\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        values.append(value)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            returns_batch = []\n",
    "            discounted_sum = 0\n",
    "            for r in rewards[::-1]:\n",
    "                discounted_sum = r + gamma * discounted_sum\n",
    "                returns_batch.append(discounted_sum)\n",
    "            returns_batch.reverse()\n",
    "\n",
    "            states = tf.concat(states, axis=0)\n",
    "            actions = np.array(actions, dtype=np.int32)\n",
    "            values = tf.concat(values, axis=0)\n",
    "            returns_batch = tf.convert_to_tensor(returns_batch)\n",
    "            old_logits, _ = model(states)\n",
    "\n",
    "            loss = ppo_loss(old_logits, values, returns_batch - np.array(values), states, actions, returns_batch)\n",
    "            # print(f\"Episode: {episode + 1}, Loss: {loss.numpy()}\")\n",
    "\n",
    "            break\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot([i for i in range(len(l))], l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from policies.policies import P1_pt\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = P1_pt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.random.rand(84, 32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (4x32 and 4x16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m p\u001b[38;5;241m.\u001b[39mforward(s)\n",
      "File \u001b[0;32m~/Desktop/mutlti_robots/policies/policies.py:222\u001b[0m, in \u001b[0;36mP1_pt.forward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    219\u001b[0m ol \u001b[38;5;241m=\u001b[39m ol\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    221\u001b[0m \u001b[38;5;66;03m# Apply fully connected layers to other inputs\u001b[39;00m\n\u001b[0;32m--> 222\u001b[0m od \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_od(od))\n\u001b[1;32m    223\u001b[0m og \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_og(og))\n\u001b[1;32m    224\u001b[0m ov \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_ov(ov))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (4x32 and 4x16)"
     ]
    }
   ],
   "source": [
    "p.forward(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from policies.policies import P1_pt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = P1_pt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ol = torch.randn(6, 64)\n",
    "od = torch.randn(6, 4)\n",
    "og = torch.randn(6, 8)\n",
    "ov = torch.randn(6, 8)\n",
    "ip = torch.randn(6, 84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [16, 1, 5], expected input[1, 6, 64] to have 1 channels, but got 6 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m p\u001b[38;5;241m.\u001b[39mvalue(ol, od, og, ov)\n",
      "File \u001b[0;32m~/Desktop/mutlti_robots/policies/policies.py:69\u001b[0m, in \u001b[0;36mP1_pt.value\u001b[0;34m(self, ol, od, og, ov)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalue\u001b[39m(\u001b[38;5;28mself\u001b[39m, ol, od, og, ov):\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# Forward pass for ol\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m     ol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_layers_ol(ol)\n\u001b[1;32m     70\u001b[0m     ol \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(ol, start_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m# Forward pass for od, og, ov\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:310\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:306\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    304\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    305\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    307\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [16, 1, 5], expected input[1, 6, 64] to have 1 channels, but got 6 channels instead"
     ]
    }
   ],
   "source": [
    "p.value(ol, od, og, ov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        # Convolutional layers for ol (laser scanner input)\n",
    "        self.conv_layers_ol = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=16, kernel_size=5, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Fully connected layers for od, og, ov\n",
    "        self.fc_od = nn.Sequential(\n",
    "            nn.Linear(in_features=4, out_features=16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_og = nn.Sequential(\n",
    "            nn.Linear(in_features=8, out_features=32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_ov = nn.Sequential(\n",
    "            nn.Linear(in_features=8, out_features=32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Fully connected layer to merge all inputs\n",
    "        self.fc_merge = nn.Sequential(\n",
    "            nn.Linear(in_features=1936, out_features=384),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Final output layer for velocity\n",
    "        self.fc_velocity = nn.Linear(in_features=384, out_features=4)\n",
    "\n",
    "    def forward(self, ol, od, og, ov):\n",
    "        # Forward pass for ol\n",
    "        ol = self.conv_layers_ol(ol)\n",
    "        ol = torch.flatten(ol, start_dim=1)\n",
    "\n",
    "        # Forward pass for od, og, ov\n",
    "        od = self.fc_od(od)\n",
    "        og = self.fc_og(og)\n",
    "        ov = self.fc_ov(ov)\n",
    "\n",
    "        # Concatenate all the fully connected layers\n",
    "        merged = torch.cat((ol, od, og, ov), dim=1)\n",
    "\n",
    "        # Apply the final fully connected layer\n",
    "        merged = self.fc_merge(merged)\n",
    "\n",
    "        # Output layer for velocity\n",
    "        velocity = self.fc_velocity(merged)\n",
    "\n",
    "        return velocity\n",
    "\n",
    "# Initialize the neural network\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from policies.policies import P1_pt\n",
    "import torch\n",
    "model = P1_pt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ol = torch.randn(512, 1, 64)  # Example input for laser scanner (1 sample, 1 channel, 10 features)\n",
    "od = torch.randn(512, 4)       # Example input for od (1 sample, 4 features)\n",
    "og = torch.randn(512, 8)       # Example input for og (1 sample, 4 features)\n",
    "ov = torch.randn(512, 8)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0817, -0.0604,  0.0264,  0.1756],\n",
       "        [-0.0932, -0.0929,  0.0796,  0.1389],\n",
       "        [-0.0336, -0.0652,  0.0401,  0.1685],\n",
       "        ...,\n",
       "        [-0.0655, -0.0098,  0.0815,  0.1175],\n",
       "        [-0.0324, -0.0354,  0.1070,  0.1550],\n",
       "        [-0.0444, -0.0591,  0.0107,  0.1592]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model.forward(ol, od, og, ov)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0817, -0.0604],\n",
       "        [-0.0932, -0.0929],\n",
       "        [-0.0336, -0.0652],\n",
       "        ...,\n",
       "        [-0.0655, -0.0098],\n",
       "        [-0.0324, -0.0354],\n",
       "        [-0.0444, -0.0591]], grad_fn=<SplitBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model.determine_actions(ol, od, og, ov)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0818, -0.2637],\n",
       "        [ 1.2565, -1.5281],\n",
       "        [-0.3413, -0.2549],\n",
       "        ...,\n",
       "        [-0.9628, -0.1468],\n",
       "        [-0.6445, -0.1651],\n",
       "        [-0.0418, -1.1836]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions = model.sample_actions(ol, od, og, ov)\n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3455e+00],\n",
       "        [-2.7155e-01],\n",
       "        [-5.9624e-01],\n",
       "        [-2.0472e+00],\n",
       "        [-1.2440e+00],\n",
       "        [ 3.1462e-02],\n",
       "        [-2.2797e+00],\n",
       "        [ 1.0753e+00],\n",
       "        [ 6.1478e-03],\n",
       "        [ 8.9549e-01],\n",
       "        [ 1.9419e-01],\n",
       "        [ 1.3053e+00],\n",
       "        [-4.5702e-01],\n",
       "        [-9.6064e-01],\n",
       "        [ 6.4749e-02],\n",
       "        [ 1.5630e-01],\n",
       "        [-1.9893e+00],\n",
       "        [-8.7359e-02],\n",
       "        [ 7.4465e-01],\n",
       "        [-1.6855e+00],\n",
       "        [-1.1030e+00],\n",
       "        [ 1.1970e+00],\n",
       "        [-2.0567e+00],\n",
       "        [ 5.8582e-02],\n",
       "        [ 1.4214e+00],\n",
       "        [-2.1386e+00],\n",
       "        [ 1.4878e+00],\n",
       "        [-9.2372e-01],\n",
       "        [-1.7371e+00],\n",
       "        [ 1.9388e+00],\n",
       "        [-1.2547e-01],\n",
       "        [-1.1128e+00],\n",
       "        [ 1.1699e+00],\n",
       "        [-9.3360e-01],\n",
       "        [ 8.3775e-01],\n",
       "        [ 4.3926e-02],\n",
       "        [ 9.3839e-01],\n",
       "        [-8.1413e-01],\n",
       "        [-1.2960e-01],\n",
       "        [-5.1779e-01],\n",
       "        [ 6.7339e-01],\n",
       "        [ 5.6401e-01],\n",
       "        [ 1.3361e+00],\n",
       "        [ 2.2221e+00],\n",
       "        [ 3.7087e-01],\n",
       "        [ 8.0082e-01],\n",
       "        [-7.9824e-01],\n",
       "        [ 3.5119e-01],\n",
       "        [-2.2785e-01],\n",
       "        [-3.3340e-01],\n",
       "        [-3.0649e-01],\n",
       "        [-1.4580e+00],\n",
       "        [ 1.0433e+00],\n",
       "        [-7.7641e-02],\n",
       "        [-8.5717e-01],\n",
       "        [-1.3040e-01],\n",
       "        [-9.5693e-01],\n",
       "        [-1.6375e-01],\n",
       "        [-1.6304e+00],\n",
       "        [-1.1733e+00],\n",
       "        [-4.2773e-01],\n",
       "        [-5.3778e-01],\n",
       "        [ 1.5523e+00],\n",
       "        [ 3.8828e-01],\n",
       "        [-9.8992e-01],\n",
       "        [ 1.3979e+00],\n",
       "        [-8.8367e-01],\n",
       "        [-6.5424e-01],\n",
       "        [ 1.1741e+00],\n",
       "        [-2.4986e+00],\n",
       "        [ 1.6756e+00],\n",
       "        [-5.0249e-01],\n",
       "        [-6.3049e-01],\n",
       "        [-1.7388e-01],\n",
       "        [ 1.1483e+00],\n",
       "        [-9.8653e-02],\n",
       "        [-1.3077e+00],\n",
       "        [-3.4340e-01],\n",
       "        [ 1.3006e+00],\n",
       "        [ 8.5437e-01],\n",
       "        [-1.0713e+00],\n",
       "        [-2.9875e-01],\n",
       "        [-1.2243e+00],\n",
       "        [ 2.4825e+00],\n",
       "        [ 1.5865e+00],\n",
       "        [ 9.2910e-01],\n",
       "        [ 4.8152e-01],\n",
       "        [-3.0150e-02],\n",
       "        [ 7.2757e-01],\n",
       "        [ 9.9764e-01],\n",
       "        [-8.5878e-01],\n",
       "        [-2.8098e-01],\n",
       "        [-2.2536e+00],\n",
       "        [ 1.5817e+00],\n",
       "        [ 1.1265e-01],\n",
       "        [ 3.5437e-01],\n",
       "        [-7.5723e-01],\n",
       "        [ 3.3373e-01],\n",
       "        [ 2.7538e+00],\n",
       "        [-6.6381e-01],\n",
       "        [ 1.3117e+00],\n",
       "        [-9.4784e-01],\n",
       "        [-6.4319e-01],\n",
       "        [-6.3710e-01],\n",
       "        [ 6.6432e-02],\n",
       "        [-1.1385e+00],\n",
       "        [-8.1397e-01],\n",
       "        [ 5.2821e-01],\n",
       "        [ 4.8646e-01],\n",
       "        [ 1.0334e+00],\n",
       "        [-1.0544e+00],\n",
       "        [-9.5297e-01],\n",
       "        [ 6.0733e-01],\n",
       "        [-1.0930e+00],\n",
       "        [-5.2345e-01],\n",
       "        [-2.3907e-02],\n",
       "        [-1.2078e+00],\n",
       "        [-6.5327e-01],\n",
       "        [-4.6485e-02],\n",
       "        [ 7.6170e-01],\n",
       "        [ 2.5634e-01],\n",
       "        [-9.3415e-01],\n",
       "        [-2.3312e-01],\n",
       "        [-1.2343e+00],\n",
       "        [-3.4514e-01],\n",
       "        [-2.0492e+00],\n",
       "        [ 3.2002e-01],\n",
       "        [ 5.7297e-01],\n",
       "        [-9.4307e-02],\n",
       "        [-6.8487e-01],\n",
       "        [ 1.2672e+00],\n",
       "        [ 2.2694e-01],\n",
       "        [ 4.6618e-02],\n",
       "        [-9.1971e-01],\n",
       "        [ 1.8442e-01],\n",
       "        [ 5.1696e-01],\n",
       "        [-3.6121e-01],\n",
       "        [ 9.5249e-03],\n",
       "        [-1.0126e+00],\n",
       "        [ 6.2006e-01],\n",
       "        [-8.3260e-01],\n",
       "        [-4.1904e-01],\n",
       "        [-2.7659e-01],\n",
       "        [-3.8103e-01],\n",
       "        [-1.4518e-01],\n",
       "        [ 9.8072e-01],\n",
       "        [-1.9192e+00],\n",
       "        [-6.3947e-01],\n",
       "        [-2.0602e+00],\n",
       "        [-1.1782e-02],\n",
       "        [ 1.0894e+00],\n",
       "        [ 2.7504e-01],\n",
       "        [-8.3234e-02],\n",
       "        [ 1.2698e+00],\n",
       "        [-1.5537e+00],\n",
       "        [ 2.0487e-01],\n",
       "        [-5.9914e-01],\n",
       "        [ 8.6079e-01],\n",
       "        [-8.3409e-03],\n",
       "        [ 1.8394e-01],\n",
       "        [-4.9446e-01],\n",
       "        [ 5.1129e-02],\n",
       "        [ 8.7953e-02],\n",
       "        [ 3.4265e-01],\n",
       "        [-7.7370e-01],\n",
       "        [-5.2447e-01],\n",
       "        [ 6.6864e-01],\n",
       "        [ 4.8291e-01],\n",
       "        [-2.7009e-01],\n",
       "        [ 3.9900e-01],\n",
       "        [-5.1590e-02],\n",
       "        [ 2.9325e-01],\n",
       "        [-5.3440e-01],\n",
       "        [-9.6657e-01],\n",
       "        [-1.9729e+00],\n",
       "        [-8.6852e-01],\n",
       "        [-7.9019e-01],\n",
       "        [-1.1726e+00],\n",
       "        [-2.0327e-01],\n",
       "        [-1.5759e+00],\n",
       "        [-1.1855e+00],\n",
       "        [ 7.8114e-01],\n",
       "        [-7.5690e-01],\n",
       "        [ 1.8243e+00],\n",
       "        [-8.1913e-01],\n",
       "        [ 3.3423e-02],\n",
       "        [ 2.9034e+00],\n",
       "        [ 9.9379e-02],\n",
       "        [ 1.8247e-01],\n",
       "        [-2.1488e+00],\n",
       "        [-4.0329e-01],\n",
       "        [-1.1604e+00],\n",
       "        [-5.7568e-01],\n",
       "        [-2.2451e-02],\n",
       "        [-5.0525e-01],\n",
       "        [-1.1226e+00],\n",
       "        [-1.3267e+00],\n",
       "        [-5.1991e-01],\n",
       "        [ 3.6069e-01],\n",
       "        [-6.5318e-02],\n",
       "        [-2.3333e-02],\n",
       "        [-7.2377e-01],\n",
       "        [ 9.4788e-01],\n",
       "        [ 9.2875e-01],\n",
       "        [ 8.3451e-01],\n",
       "        [-4.4139e-02],\n",
       "        [ 2.1085e-01],\n",
       "        [-3.8557e-02],\n",
       "        [-1.4702e+00],\n",
       "        [-1.8353e+00],\n",
       "        [-2.0080e-01],\n",
       "        [ 2.8253e+00],\n",
       "        [ 6.0228e-01],\n",
       "        [-1.3011e+00],\n",
       "        [-1.4245e+00],\n",
       "        [-2.6554e-01],\n",
       "        [-1.8086e+00],\n",
       "        [ 1.0557e+00],\n",
       "        [ 2.3950e-01],\n",
       "        [ 6.0603e-01],\n",
       "        [ 1.0895e+00],\n",
       "        [-1.0324e+00],\n",
       "        [ 1.1990e+00],\n",
       "        [-2.2788e+00],\n",
       "        [ 1.3351e+00],\n",
       "        [-1.0918e+00],\n",
       "        [-2.5165e+00],\n",
       "        [-1.1088e+00],\n",
       "        [-1.2577e+00],\n",
       "        [-1.4801e+00],\n",
       "        [-7.0008e-01],\n",
       "        [-2.0732e+00],\n",
       "        [ 5.6376e-01],\n",
       "        [-9.9503e-01],\n",
       "        [-5.8563e-01],\n",
       "        [-5.8872e-01],\n",
       "        [ 1.1839e+00],\n",
       "        [-1.6885e+00],\n",
       "        [-1.9289e-01],\n",
       "        [-7.6913e-01],\n",
       "        [-1.9990e-01],\n",
       "        [-1.0741e+00],\n",
       "        [ 9.7742e-01],\n",
       "        [-8.1998e-01],\n",
       "        [-1.1924e+00],\n",
       "        [-6.5348e-01],\n",
       "        [ 2.3951e-01],\n",
       "        [-3.5540e-01],\n",
       "        [-4.9130e-02],\n",
       "        [-1.7346e+00],\n",
       "        [ 3.0235e-01],\n",
       "        [-2.2650e-01],\n",
       "        [-1.3008e+00],\n",
       "        [ 7.3598e-01],\n",
       "        [-1.4146e+00],\n",
       "        [-6.2020e-01],\n",
       "        [ 7.8072e-01],\n",
       "        [-4.5379e-01],\n",
       "        [ 1.3235e+00],\n",
       "        [-5.6612e-01],\n",
       "        [-2.8404e+00],\n",
       "        [-2.0634e-01],\n",
       "        [ 1.2027e-01],\n",
       "        [-1.0703e+00],\n",
       "        [-2.1913e-01],\n",
       "        [-8.1356e-01],\n",
       "        [-4.1777e-01],\n",
       "        [ 6.8934e-01],\n",
       "        [ 3.8612e-01],\n",
       "        [-1.5073e-01],\n",
       "        [-4.8991e-01],\n",
       "        [ 1.1112e+00],\n",
       "        [-6.7853e-01],\n",
       "        [ 4.8666e-01],\n",
       "        [-1.2751e+00],\n",
       "        [ 3.4936e-01],\n",
       "        [-6.2083e-01],\n",
       "        [ 1.2333e+00],\n",
       "        [ 1.3840e+00],\n",
       "        [-8.8046e-01],\n",
       "        [-2.7119e-01],\n",
       "        [ 9.1550e-01],\n",
       "        [ 1.1791e+00],\n",
       "        [-1.8389e+00],\n",
       "        [-1.6358e+00],\n",
       "        [-1.9768e+00],\n",
       "        [ 6.2749e-01],\n",
       "        [ 1.2245e+00],\n",
       "        [-5.6725e-01],\n",
       "        [ 1.2503e+00],\n",
       "        [-7.7239e-01],\n",
       "        [ 1.5383e-01],\n",
       "        [-1.7112e+00],\n",
       "        [-2.4164e-01],\n",
       "        [-3.0830e-02],\n",
       "        [ 3.1697e-01],\n",
       "        [-1.6986e-01],\n",
       "        [-1.0357e+00],\n",
       "        [-2.7098e+00],\n",
       "        [ 1.1213e+00],\n",
       "        [-1.8855e-01],\n",
       "        [ 1.2555e-01],\n",
       "        [ 9.5285e-02],\n",
       "        [ 1.8777e+00],\n",
       "        [ 1.2754e-01],\n",
       "        [ 8.8874e-02],\n",
       "        [ 1.9413e+00],\n",
       "        [-1.2480e+00],\n",
       "        [-1.6292e-01],\n",
       "        [ 4.6011e-01],\n",
       "        [-2.1355e-01],\n",
       "        [ 3.0659e-02],\n",
       "        [ 9.7487e-02],\n",
       "        [ 9.8085e-01],\n",
       "        [-1.5629e-01],\n",
       "        [-1.3296e+00],\n",
       "        [ 3.3591e-01],\n",
       "        [-1.9095e+00],\n",
       "        [-5.9645e-01],\n",
       "        [ 4.1248e-01],\n",
       "        [-1.4352e+00],\n",
       "        [-1.7535e-01],\n",
       "        [-1.2088e+00],\n",
       "        [ 1.5795e+00],\n",
       "        [ 7.7013e-01],\n",
       "        [-4.0782e-01],\n",
       "        [ 8.8552e-02],\n",
       "        [-2.2328e-01],\n",
       "        [ 9.3903e-01],\n",
       "        [-7.4071e-01],\n",
       "        [-8.4129e-01],\n",
       "        [-1.4891e+00],\n",
       "        [-2.7007e-01],\n",
       "        [ 1.9629e+00],\n",
       "        [ 1.5446e+00],\n",
       "        [ 2.2393e+00],\n",
       "        [ 1.0147e+00],\n",
       "        [-4.4890e-02],\n",
       "        [ 3.3164e+00],\n",
       "        [ 4.7611e-01],\n",
       "        [-4.8569e-01],\n",
       "        [-2.3842e+00],\n",
       "        [-1.4386e+00],\n",
       "        [-3.9420e-01],\n",
       "        [ 1.1651e+00],\n",
       "        [-1.8766e+00],\n",
       "        [ 6.2423e-01],\n",
       "        [ 3.9467e-01],\n",
       "        [ 8.7176e-01],\n",
       "        [ 5.5408e-02],\n",
       "        [-7.4372e-01],\n",
       "        [ 4.3776e-01],\n",
       "        [-2.6898e-01],\n",
       "        [-4.1440e-01],\n",
       "        [-1.4833e+00],\n",
       "        [-1.4037e+00],\n",
       "        [-1.3515e+00],\n",
       "        [ 2.0034e+00],\n",
       "        [-1.1322e+00],\n",
       "        [-1.2547e+00],\n",
       "        [-9.7922e-01],\n",
       "        [-6.1352e-01],\n",
       "        [-1.3298e+00],\n",
       "        [-1.5622e-01],\n",
       "        [-1.8426e+00],\n",
       "        [-2.4361e-02],\n",
       "        [ 1.6153e+00],\n",
       "        [ 8.4115e-01],\n",
       "        [-2.1394e-01],\n",
       "        [-8.3208e-01],\n",
       "        [ 1.8747e+00],\n",
       "        [ 2.2355e-01],\n",
       "        [ 9.2291e-01],\n",
       "        [-1.4272e+00],\n",
       "        [ 4.3861e-01],\n",
       "        [-9.1503e-03],\n",
       "        [-1.3455e+00],\n",
       "        [-4.6562e-01],\n",
       "        [-2.2268e-01],\n",
       "        [-4.1076e-01],\n",
       "        [ 6.6982e-01],\n",
       "        [ 6.0317e-01],\n",
       "        [ 8.9710e-02],\n",
       "        [-2.0277e-01],\n",
       "        [-4.6494e-02],\n",
       "        [-1.6807e+00],\n",
       "        [-6.4214e-02],\n",
       "        [-3.5753e-01],\n",
       "        [ 3.3146e-01],\n",
       "        [-2.0935e+00],\n",
       "        [ 5.8196e-01],\n",
       "        [-2.1178e+00],\n",
       "        [ 2.6115e-01],\n",
       "        [ 8.0410e-01],\n",
       "        [ 4.9719e-01],\n",
       "        [-4.3163e-01],\n",
       "        [-2.8716e-01],\n",
       "        [ 1.3304e+00],\n",
       "        [-1.7818e-01],\n",
       "        [ 1.4107e+00],\n",
       "        [-1.0963e-03],\n",
       "        [ 3.4163e-01],\n",
       "        [ 2.0348e+00],\n",
       "        [ 1.8926e-01],\n",
       "        [-1.6307e+00],\n",
       "        [ 5.3514e-01],\n",
       "        [-7.0716e-01],\n",
       "        [ 4.0739e-01],\n",
       "        [-1.5762e+00],\n",
       "        [ 1.6551e-01],\n",
       "        [-3.3709e-01],\n",
       "        [-2.1957e-01],\n",
       "        [-1.3664e+00],\n",
       "        [-4.5991e-01],\n",
       "        [ 3.5233e-01],\n",
       "        [ 1.1416e+00],\n",
       "        [-1.7098e-01],\n",
       "        [-6.9992e-01],\n",
       "        [ 3.0518e-01],\n",
       "        [-5.4952e-01],\n",
       "        [ 1.2582e+00],\n",
       "        [-3.8713e-01],\n",
       "        [ 3.6844e-01],\n",
       "        [-7.3174e-01],\n",
       "        [-6.1297e-01],\n",
       "        [ 1.3783e+00],\n",
       "        [-3.6277e-01],\n",
       "        [-3.8207e-01],\n",
       "        [-1.0861e+00],\n",
       "        [ 9.6562e-01],\n",
       "        [-9.1180e-01],\n",
       "        [ 2.9504e+00],\n",
       "        [-1.1783e+00],\n",
       "        [-2.0541e+00],\n",
       "        [-2.2189e+00],\n",
       "        [ 1.3099e+00],\n",
       "        [-1.4016e-01],\n",
       "        [-7.0305e-01],\n",
       "        [-1.9197e-02],\n",
       "        [-3.4148e-01],\n",
       "        [ 5.1823e-01],\n",
       "        [-9.1666e-01],\n",
       "        [ 2.3405e+00],\n",
       "        [-8.3467e-01],\n",
       "        [-1.2523e+00],\n",
       "        [-7.5817e-01],\n",
       "        [-1.7860e+00],\n",
       "        [-6.5248e-01],\n",
       "        [ 1.5619e-01],\n",
       "        [ 2.1190e+00],\n",
       "        [-5.4001e-01],\n",
       "        [ 9.2954e-01],\n",
       "        [ 5.9087e-01],\n",
       "        [ 2.9150e+00],\n",
       "        [-1.4441e+00],\n",
       "        [-2.4943e-01],\n",
       "        [ 1.4921e+00],\n",
       "        [-8.6169e-02],\n",
       "        [ 3.4660e-01],\n",
       "        [-9.4687e-01],\n",
       "        [-5.6583e-01],\n",
       "        [-2.2041e+00],\n",
       "        [ 4.7950e-01],\n",
       "        [ 2.1690e-01],\n",
       "        [ 1.3830e-01],\n",
       "        [-1.0249e+00],\n",
       "        [ 7.1027e-01],\n",
       "        [ 2.0756e+00],\n",
       "        [-3.5201e-02],\n",
       "        [-6.9590e-01],\n",
       "        [-2.4623e+00],\n",
       "        [-5.7989e-01],\n",
       "        [ 6.8632e-02],\n",
       "        [ 8.5099e-01],\n",
       "        [-3.2386e-01],\n",
       "        [ 9.1772e-01],\n",
       "        [ 7.3788e-01],\n",
       "        [-1.5894e+00],\n",
       "        [ 2.7045e-01],\n",
       "        [-6.7613e-01],\n",
       "        [ 1.4555e-01],\n",
       "        [-1.0348e+00],\n",
       "        [-1.2109e-02],\n",
       "        [-8.1062e-01],\n",
       "        [ 2.3754e+00],\n",
       "        [-1.5237e+00],\n",
       "        [ 2.2379e-01],\n",
       "        [ 1.5982e+00],\n",
       "        [-4.3942e-01],\n",
       "        [-9.9772e-01],\n",
       "        [ 4.2260e-01],\n",
       "        [ 2.8488e-01],\n",
       "        [-2.3762e-01],\n",
       "        [-3.6427e-01],\n",
       "        [-9.9048e-01],\n",
       "        [ 7.5219e-01],\n",
       "        [-1.6222e-01],\n",
       "        [-2.6614e-01],\n",
       "        [-2.3747e-01],\n",
       "        [ 2.7447e-01],\n",
       "        [-7.3816e-01],\n",
       "        [-8.6355e-01],\n",
       "        [ 5.0507e-01],\n",
       "        [ 1.1563e-01],\n",
       "        [ 1.0442e+00],\n",
       "        [-1.1189e-01],\n",
       "        [-2.2113e-01],\n",
       "        [ 1.0623e-01],\n",
       "        [-1.4909e-01],\n",
       "        [-1.1096e+00],\n",
       "        [-8.0964e-01],\n",
       "        [-1.2254e+00]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions.sum(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = model.forward(ol, od, og, ov)  # map states to distribution parameters\n",
    "mu, sigma = torch.chunk(params, 2, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 2])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 2])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8151, -0.5643],\n",
       "        [-2.0868, -2.2856],\n",
       "        [-0.6342, -0.8949],\n",
       "        ...,\n",
       "        [-0.8599, -0.7298],\n",
       "        [-0.8989, -0.5394],\n",
       "        [-2.2717, -0.4689]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.log_prob(actions, ol, od, og, ov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.11.5)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from env import Environment\n",
    "from robots import CarLikeBot, CircleRobot\n",
    "import numpy as np\n",
    "import torch \n",
    "from policies.policies import P1_pt\n",
    "from rewards import RewardCircle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "p = P1_pt()\n",
    "env = Environment(CircleRobot, 3, reward=RewardCircle())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_trajectories(env, pi, max_steps):    \n",
    "    ol = np.zeros((max_steps + 1, 1, 64), dtype=float)\n",
    "    od = np.zeros((max_steps + 1, 4), dtype=float)\n",
    "    og = np.zeros((max_steps + 1, 8), dtype=float)\n",
    "    ov = np.zeros((max_steps + 1, 8), dtype=float)  # states from s(0) to s(T+1)\n",
    "    actions = np.zeros((max_steps, 2), dtype=float)  # actions from a(0) to a(T)\n",
    "    rewards = np.zeros((max_steps), dtype=float)  # rewards from r(0) to r(T)\n",
    "    env.reset()\n",
    "    ol[0], od[0], og[0], ov[0] = env.get_observation()\n",
    "    for t in range(max_steps):\n",
    "        \n",
    "        a = pi.sample_actions(torch.tensor(np.reshape(ol[t], (1, 1, 64))).float(),\n",
    "                              torch.tensor(np.reshape(od[t], (1, 4))).float(),\n",
    "                              torch.tensor(np.reshape(og[t], (1, 8))).float(),\n",
    "                              torch.tensor(np.reshape(ov[t], (1, 8))).float())\n",
    "\n",
    "        ol_next, od_next, og_next, ov_next, r, done = env.step(np.array(a))  # env needs numpy array of (Nx1)\n",
    "\n",
    "        ol[t + 1] = ol_next\n",
    "        od[t + 1] = od_next\n",
    "        og[t + 1] = og_next\n",
    "        ov[t + 1] = ov_next\n",
    "        actions[t] = a\n",
    "        rewards[t] = r[0]\n",
    "        \n",
    "    ol = torch.tensor(ol).float()\n",
    "    od = torch.tensor(od).float()\n",
    "    og = torch.tensor(og).float()\n",
    "    ov = torch.tensor(ov).float()   # (T+1, N, state_dim)  care for the extra timestep at the end!\n",
    "    tensor_a = torch.tensor(actions).float()  # (T, N, 1)\n",
    "    tensor_r = torch.tensor(rewards).float()  # (T, N)\n",
    "    \n",
    "    return ol, od, og, ov, tensor_a, tensor_r\n",
    "\n",
    "def discount_cum_sum(rewards, gamma):\n",
    "    T = rewards.shape[0]\n",
    "    returns = torch.zeros_like(rewards)\n",
    "    for t in range(T):\n",
    "        # Compute the discounted sum of future rewards from time step t\n",
    "        returns[t] = torch.sum(rewards[t:] * (gamma ** torch.arange(T - t)))\n",
    "    return returns\n",
    "\n",
    "def compute_gae(tensor_r, values, gamma, lambda_):\n",
    "    delta_t = tensor_r + gamma * values[1:] - values[:-1]\n",
    "    advantages = discount_cum_sum(delta_t, gamma * lambda_)\n",
    "    value_targets = advantages + values[:-1]\n",
    "    return value_targets, advantages\n",
    "\n",
    "def value_loss(values, value_targets):\n",
    "    value_loss = torch.tensor(0.)\n",
    "    T = values.shape[0]\n",
    "    value_loss += (1/T)*torch.sum((values - value_targets) ** 2)\n",
    "    return value_loss\n",
    "\n",
    "def ppo_loss(p_ratios, advantage_estimates, epsilon):\n",
    "    policy_loss = torch.tensor(0.) \n",
    "    T = p_ratios.shape[0]\n",
    "    \n",
    "    p_ratios = p_ratios.sum(dim=1)\n",
    "    clipped_ratios = torch.clamp(p_ratios, 1.0 - epsilon, 1.0 + epsilon)\n",
    "\n",
    "    surrogate_loss = torch.min(p_ratios * advantage_estimates, clipped_ratios * advantage_estimates)\n",
    "    policy_loss -= (1/T)*torch.sum(surrogate_loss)\n",
    "    return policy_loss\n",
    "    \n",
    "def plot_training(rewards, p_losses, v_losses=None):\n",
    "    num_plots = 2 if v_losses is None else 3\n",
    "\n",
    "    plt.subplot(num_plots, 1, 1)\n",
    "    plt.plot(rewards, label='mean rewards', color='green')\n",
    "    plt.ylabel('Mean reward')\n",
    "    plt.subplot(num_plots, 1, 2)\n",
    "    plt.plot(p_losses, label='policy loss', color='red')\n",
    "    plt.ylabel('Policy loss')\n",
    "    if v_losses is not None:\n",
    "        plt.subplot(num_plots, 1, 3)\n",
    "        plt.plot(v_losses, label='value loss', color='blue')\n",
    "        plt.ylabel('Value loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100]) torch.Size([101])\n",
      "torch.Size([100]) torch.Size([101])\n",
      "torch.Size([100]) torch.Size([101])\n",
      "torch.Size([100]) torch.Size([101])\n",
      "torch.Size([100]) torch.Size([101])\n",
      "torch.Size([100]) torch.Size([101])\n",
      "torch.Size([100]) torch.Size([101])\n",
      "torch.Size([100]) torch.Size([101])\n",
      "torch.Size([100]) torch.Size([101])\n",
      "torch.Size([100]) torch.Size([101])\n",
      "Epoch 0, mean reward: -0.174, value loss: 3993.669\n",
      "torch.Size([100]) torch.Size([101])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m     logp_old \u001b[38;5;241m=\u001b[39m pi\u001b[38;5;241m.\u001b[39mlog_prob(tensor_a, ol[:max_steps], od[:max_steps], og[:max_steps], ov[:max_steps])\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(sgd_iters):\n\u001b[0;32m---> 23\u001b[0m     values \u001b[38;5;241m=\u001b[39m pi\u001b[38;5;241m.\u001b[39mvalue_estimates(ol, od, og, ov)\n\u001b[1;32m     24\u001b[0m     logp \u001b[38;5;241m=\u001b[39m pi\u001b[38;5;241m.\u001b[39mlog_prob(tensor_a, ol[:max_steps], od[:max_steps], og[:max_steps], ov[:max_steps])\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/Desktop/mutlti_robots/policies/policies.py:116\u001b[0m, in \u001b[0;36mP1_pt.value_estimates\u001b[0;34m(self, ol, od, og, ov)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalue_estimates\u001b[39m(\u001b[38;5;28mself\u001b[39m, ol, od, og, ov):\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue(ol, od, og, ov)\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "File \u001b[0;32m~/Desktop/mutlti_robots/policies/policies.py:73\u001b[0m, in \u001b[0;36mP1_pt.value\u001b[0;34m(self, ol, od, og, ov)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalue\u001b[39m(\u001b[38;5;28mself\u001b[39m, ol, od, og, ov):\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m# Forward pass for ol\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     ol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_layers_ol(ol)\n\u001b[1;32m     74\u001b[0m     ol \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(ol, start_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;66;03m# Forward pass for od, og, ov\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:310\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:306\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    304\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    305\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    307\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_steps = 100\n",
    "epochs = 5000\n",
    "lr = 0.0003\n",
    "gamma=0.95\n",
    "epsilon = 0.2\n",
    "\n",
    "sgd_iters = 10\n",
    "\n",
    "pi = P1_pt()\n",
    "pi.load_state_dict(torch.load('model.pth'))\n",
    "train_env = Environment(CarLikeBot, 3, reward=RewardCircle())\n",
    "optim = torch.optim.Adam(pi.parameters(), lr=lr)\n",
    "\n",
    "mean_rewards, p_losses, v_losses = np.zeros(epochs), np.zeros(epochs), np.zeros(epochs)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    ol, od, og, ov, tensor_a, tensor_r = sample_trajectories(train_env, pi, max_steps)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logp_old = pi.log_prob(tensor_a, ol[:max_steps], od[:max_steps], og[:max_steps], ov[:max_steps])\n",
    "\n",
    "    for i in range(sgd_iters):\n",
    "        values = pi.value_estimates(ol, od, og, ov)\n",
    "        logp = pi.log_prob(tensor_a, ol[:max_steps], od[:max_steps], og[:max_steps], ov[:max_steps])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            print(tensor_r.shape, values.shape)\n",
    "            value_targets, advantage_estimates = compute_gae(tensor_r, values, gamma, lambda_=0.97)\n",
    "            advantage_estimates = (advantage_estimates - advantage_estimates.mean()) / advantage_estimates.std()\n",
    "                \n",
    "        L_v = value_loss(values[:max_steps], value_targets)\n",
    "        \n",
    "        p_ratios = torch.exp(logp - logp_old)\n",
    "    \n",
    "        L_ppo = ppo_loss(p_ratios, advantage_estimates, epsilon=epsilon)\n",
    "        total_loss = L_v + L_ppo\n",
    "            \n",
    "        optim.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch %d, mean reward: %.3f, value loss: %.3f' % (epoch, tensor_r.mean(), L_v.item()))\n",
    "    mean_rewards[epoch] = tensor_r.mean()\n",
    "    v_losses[epoch] = L_v.item()\n",
    "    p_losses[epoch] = L_ppo.item()\n",
    "    # raise\n",
    "        \n",
    "# train_env.close()\n",
    "\n",
    "plot_training(mean_rewards, p_losses, v_losses)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.11.5)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from env import Environment\n",
    "from robots import CarLikeBot, CircleRobot\n",
    "import numpy as np\n",
    "import torch \n",
    "from policies.policies import P1_pt\n",
    "from rewards import RewardCircle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "p = P1_pt()\n",
    "env = Environment(CarLikeBot, 3, reward=RewardCircle())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_trajectories(env, pi, max_steps):    \n",
    "    ol = np.zeros((max_steps + 1, 1, 64), dtype=float)\n",
    "    od = np.zeros((max_steps + 1, 4), dtype=float)\n",
    "    og = np.zeros((max_steps + 1, 8), dtype=float)\n",
    "    ov = np.zeros((max_steps + 1, 8), dtype=float)  # states from s(0) to s(T+1)\n",
    "    actions = np.zeros((max_steps, 2), dtype=float)  # actions from a(0) to a(T)\n",
    "    rewards = np.zeros((max_steps), dtype=float)  # rewards from r(0) to r(T)\n",
    "    env.reset()\n",
    "    ol[0], od[0], og[0], ov[0] = env.get_observation()\n",
    "    steps = max_steps\n",
    "    for t in range(max_steps):\n",
    "        \n",
    "        a = pi.sample_actions(torch.tensor(np.reshape(ol[t], (1, 1, 64))).float(),\n",
    "                              torch.tensor(np.reshape(od[t], (1, 4))).float(),\n",
    "                              torch.tensor(np.reshape(og[t], (1, 8))).float(),\n",
    "                              torch.tensor(np.reshape(ov[t], (1, 8))).float())\n",
    "\n",
    "        ol_next, od_next, og_next, ov_next, r, done = env.step(np.array(a))  # env needs numpy array of (Nx1)\n",
    "\n",
    "        ol[t + 1] = ol_next\n",
    "        od[t + 1] = od_next\n",
    "        og[t + 1] = og_next\n",
    "        ov[t + 1] = ov_next\n",
    "        actions[t] = a\n",
    "        rewards[t] = r[0]\n",
    "        if done:\n",
    "            steps = t+1\n",
    "            break\n",
    "        \n",
    "    ol = torch.tensor(ol).float()\n",
    "    od = torch.tensor(od).float()\n",
    "    og = torch.tensor(og).float()\n",
    "    ov = torch.tensor(ov).float()\n",
    "    tensor_a = torch.tensor(actions).float()\n",
    "    tensor_r = torch.tensor(rewards).float()\n",
    "    \n",
    "    return ol, od, og, ov, tensor_a, tensor_r, steps\n",
    "\n",
    "def discount_cum_sum(rewards, gamma):\n",
    "    T = rewards.shape[0]\n",
    "    returns = torch.zeros_like(rewards)\n",
    "    for t in range(T):\n",
    "        # Compute the discounted sum of future rewards from time step t\n",
    "        returns[t] = torch.sum(rewards[t:] * (gamma ** torch.arange(T - t)))\n",
    "    return returns\n",
    "\n",
    "def compute_gae(tensor_r, values, gamma, lambda_):\n",
    "    delta_t = tensor_r + gamma * values[1:] - values[:-1]\n",
    "    advantages = discount_cum_sum(delta_t, gamma * lambda_)\n",
    "    value_targets = advantages + values[:-1]\n",
    "    return value_targets, advantages\n",
    "\n",
    "def value_loss(values, value_targets):\n",
    "    value_loss = torch.tensor(0.)\n",
    "    T = values.shape[0]\n",
    "    value_loss += (1/T)*torch.sum((values - value_targets) ** 2)\n",
    "    return value_loss\n",
    "\n",
    "def ppo_loss(p_ratios, advantage_estimates, epsilon):\n",
    "    policy_loss = torch.tensor(0.) \n",
    "    T = p_ratios.shape[0]\n",
    "    \n",
    "    p_ratios = p_ratios.sum(dim=1)\n",
    "    clipped_ratios = torch.clamp(p_ratios, 1.0 - epsilon, 1.0 + epsilon)\n",
    "\n",
    "    surrogate_loss = torch.min(p_ratios * advantage_estimates, clipped_ratios * advantage_estimates)\n",
    "    policy_loss -= (1/T)*torch.sum(surrogate_loss)\n",
    "    return policy_loss\n",
    "    \n",
    "def plot_training(rewards, p_losses, v_losses=None):\n",
    "    num_plots = 2 if v_losses is None else 3\n",
    "\n",
    "    plt.subplot(num_plots, 1, 1)\n",
    "    plt.plot(rewards, label='mean rewards', color='green')\n",
    "    plt.ylabel('Mean reward')\n",
    "    plt.subplot(num_plots, 1, 2)\n",
    "    plt.plot(p_losses, label='policy loss', color='red')\n",
    "    plt.ylabel('Policy loss')\n",
    "    if v_losses is not None:\n",
    "        plt.subplot(num_plots, 1, 3)\n",
    "        plt.plot(v_losses, label='value loss', color='blue')\n",
    "        plt.ylabel('Value loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, steps: 512 mean reward: 9.064, value loss: 47037.664\n",
      "Epoch 100, steps: 68 mean reward: 1.480, value loss: 10523.592\n",
      "Epoch 200, steps: 235 mean reward: 3.351, value loss: 2787.813\n",
      "Epoch 300, steps: 123 mean reward: 2.137, value loss: 1469.827\n",
      "Epoch 400, steps: 512 mean reward: 32.818, value loss: 112029.602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kajratbekbolinov/Desktop/mutlti_robots/env.py:15: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, steps: 189 mean reward: 6.728, value loss: 13611.232\n",
      "Epoch 600, steps: 512 mean reward: 21.647, value loss: 52434.195\n",
      "Epoch 700, steps: 512 mean reward: 19.157, value loss: 47634.867\n",
      "Epoch 800, steps: 512 mean reward: 14.742, value loss: 45601.363\n",
      "Epoch 900, steps: 512 mean reward: 13.446, value loss: 39240.434\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m mean_rewards, p_losses, v_losses \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(epochs), np\u001b[38;5;241m.\u001b[39mzeros(epochs), np\u001b[38;5;241m.\u001b[39mzeros(epochs)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 17\u001b[0m     ol, od, og, ov, tensor_a, tensor_r, steps \u001b[38;5;241m=\u001b[39m sample_trajectories(train_env, pi, max_steps)\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# print(steps)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "Cell \u001b[0;32mIn[2], line 18\u001b[0m, in \u001b[0;36msample_trajectories\u001b[0;34m(env, pi, max_steps)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_steps):\n\u001b[1;32m     13\u001b[0m     a \u001b[38;5;241m=\u001b[39m pi\u001b[38;5;241m.\u001b[39msample_actions(torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39mreshape(ol[t], (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m64\u001b[39m)))\u001b[38;5;241m.\u001b[39mfloat(),\n\u001b[1;32m     14\u001b[0m                           torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39mreshape(od[t], (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)))\u001b[38;5;241m.\u001b[39mfloat(),\n\u001b[1;32m     15\u001b[0m                           torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39mreshape(og[t], (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m8\u001b[39m)))\u001b[38;5;241m.\u001b[39mfloat(),\n\u001b[1;32m     16\u001b[0m                           torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39mreshape(ov[t], (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m8\u001b[39m)))\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[0;32m---> 18\u001b[0m     ol_next, od_next, og_next, ov_next, r, done \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(np\u001b[38;5;241m.\u001b[39marray(a))  \u001b[38;5;66;03m# env needs numpy array of (Nx1)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     ol[t \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m ol_next\n\u001b[1;32m     21\u001b[0m     od[t \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m od_next\n",
      "File \u001b[0;32m~/Desktop/mutlti_robots/env.py:178\u001b[0m, in \u001b[0;36mEnvironment.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    176\u001b[0m                 done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    177\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 178\u001b[0m ol, od, og, ov \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_observation()\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward:\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ag \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magents:\n",
      "File \u001b[0;32m~/Desktop/mutlti_robots/env.py:141\u001b[0m, in \u001b[0;36mEnvironment.get_observation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ag \u001b[38;5;241m!=\u001b[39m agent:\n\u001b[1;32m    137\u001b[0m         obj\u001b[38;5;241m.\u001b[39mappend(ag\u001b[38;5;241m.\u001b[39mget_())\n\u001b[1;32m    139\u001b[0m obs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m#  lasers\u001b[39;00m\n\u001b[0;32m--> 141\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mol\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(np\u001b[38;5;241m.\u001b[39marray(ray_cast(agent, obj)) \u001b[38;5;241m-\u001b[39m agent\u001b[38;5;241m.\u001b[39mlaser_position(), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;66;03m#  relative orientation to the goal as dir vector\u001b[39;00m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mog\u001b[39m\u001b[38;5;124m\"\u001b[39m: (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgoals[agent] \u001b[38;5;241m-\u001b[39m agent\u001b[38;5;241m.\u001b[39mposition)\u001b[38;5;241m/\u001b[39mnp\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgoals[agent] \u001b[38;5;241m-\u001b[39m agent\u001b[38;5;241m.\u001b[39mposition),\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m#  linear and angular velocity \u001b[39;00m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mov\u001b[39m\u001b[38;5;124m\"\u001b[39m: agent\u001b[38;5;241m.\u001b[39mvelocity,\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;66;03m#  euclidian dstance to goal.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mod\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mreshape(np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(agent\u001b[38;5;241m.\u001b[39mposition\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgoals[agent]), (\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    148\u001b[0m }\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m agent\u001b[38;5;241m.\u001b[39mhistory:\n\u001b[1;32m    151\u001b[0m     agent\u001b[38;5;241m.\u001b[39mhistory\u001b[38;5;241m.\u001b[39madd(obs)\n",
      "File \u001b[0;32m~/Desktop/mutlti_robots/env.py:49\u001b[0m, in \u001b[0;36mray_cast\u001b[0;34m(robot, objects)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(robot\u001b[38;5;241m.\u001b[39mlasers):\n\u001b[1;32m     48\u001b[0m     laser \u001b[38;5;241m=\u001b[39m rotation(laser, robot\u001b[38;5;241m.\u001b[39mresolution, laser_pos)\n\u001b[0;32m---> 49\u001b[0m     intersections \u001b[38;5;241m=\u001b[39m [laser_intersection((laser_pos, laser), obj) \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m objects]\n\u001b[1;32m     50\u001b[0m     ret\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mmin\u001b[39m(intersections, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(laser_pos \u001b[38;5;241m-\u001b[39m x)))\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/Desktop/mutlti_robots/env.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(robot\u001b[38;5;241m.\u001b[39mlasers):\n\u001b[1;32m     48\u001b[0m     laser \u001b[38;5;241m=\u001b[39m rotation(laser, robot\u001b[38;5;241m.\u001b[39mresolution, laser_pos)\n\u001b[0;32m---> 49\u001b[0m     intersections \u001b[38;5;241m=\u001b[39m [laser_intersection((laser_pos, laser), obj) \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m objects]\n\u001b[1;32m     50\u001b[0m     ret\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mmin\u001b[39m(intersections, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(laser_pos \u001b[38;5;241m-\u001b[39m x)))\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/Desktop/mutlti_robots/env.py:26\u001b[0m, in \u001b[0;36mlaser_intersection\u001b[0;34m(laser, object)\u001b[0m\n\u001b[1;32m     24\u001b[0m     intersection \u001b[38;5;241m=\u001b[39m circle\u001b[38;5;241m.\u001b[39mintersection(segment)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 26\u001b[0m     polygon \u001b[38;5;241m=\u001b[39m Polygon(\u001b[38;5;28mobject\u001b[39m)\n\u001b[1;32m     27\u001b[0m     intersection \u001b[38;5;241m=\u001b[39m polygon\u001b[38;5;241m.\u001b[39mintersection(segment)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m intersection\u001b[38;5;241m.\u001b[39mgeom_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPoint\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/shapely/geometry/polygon.py:230\u001b[0m, in \u001b[0;36mPolygon.__new__\u001b[0;34m(self, shell, holes)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m shell\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 230\u001b[0m     shell \u001b[38;5;241m=\u001b[39m LinearRing(shell)\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m holes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(holes) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    234\u001b[0m         \u001b[38;5;66;03m# shapely constructor cannot handle holes=[]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/shapely/geometry/polygon.py:104\u001b[0m, in \u001b[0;36mLinearRing.__new__\u001b[0;34m(self, coordinates)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(coordinates) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# empty geometry\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m# TODO better constructor + should shapely.linearrings handle this?\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m shapely\u001b[38;5;241m.\u001b[39mfrom_wkt(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLINEARRING EMPTY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 104\u001b[0m geom \u001b[38;5;241m=\u001b[39m shapely\u001b[38;5;241m.\u001b[39mlinearrings(coordinates)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(geom, LinearRing):\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid values passed to LinearRing constructor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/shapely/decorators.py:77\u001b[0m, in \u001b[0;36mmultithreading_enabled.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m array_args:\n\u001b[1;32m     76\u001b[0m         arr\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m arr, old_flag \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(array_args, old_flags):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/shapely/creation.py:171\u001b[0m, in \u001b[0;36mlinearrings\u001b[0;34m(coords, y, z, indices, out, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m coords \u001b[38;5;241m=\u001b[39m _xyz_to_coords(coords, y, z)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mlinearrings(coords, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m simple_geometries_1d(coords, indices, GeometryType\u001b[38;5;241m.\u001b[39mLINEARRING, out\u001b[38;5;241m=\u001b[39mout)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_steps = 512\n",
    "epochs = 5000\n",
    "lr = 0.0003\n",
    "gamma=0.95\n",
    "epsilon = 0.2\n",
    "\n",
    "sgd_iters = 10\n",
    "\n",
    "pi = P1_pt()\n",
    "# pi.load_state_dict(torch.load('model1.pth'))\n",
    "train_env = Environment(CarLikeBot, 3, reward=RewardCircle())\n",
    "optim = torch.optim.Adam(pi.parameters(), lr=lr)\n",
    "\n",
    "mean_rewards, p_losses, v_losses = np.zeros(epochs), np.zeros(epochs), np.zeros(epochs)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    ol, od, og, ov, tensor_a, tensor_r, steps = sample_trajectories(train_env, pi, max_steps)\n",
    "    # print(steps)\n",
    "    with torch.no_grad():\n",
    "        logp_old = pi.log_prob(tensor_a[:steps], ol[:steps], od[:steps], og[:steps], ov[:steps])\n",
    "\n",
    "    for i in range(sgd_iters):\n",
    "        values = pi.value_estimates(ol, od, og, ov)\n",
    "        logp = pi.log_prob(tensor_a[:steps], ol[:steps], od[:steps], og[:steps], ov[:steps])\n",
    "        # print(steps)\n",
    "        with torch.no_grad():\n",
    "            # print(tensor_r.shape, values.shape)\n",
    "            value_targets, advantage_estimates = compute_gae(tensor_r[:steps], values[:steps+1], gamma, lambda_=0.97)\n",
    "            advantage_estimates = (advantage_estimates - advantage_estimates.mean()) / advantage_estimates.std()\n",
    "                \n",
    "        L_v = value_loss(values[:steps], value_targets)\n",
    "        \n",
    "        p_ratios = torch.exp(logp - logp_old)\n",
    "    \n",
    "        L_ppo = ppo_loss(p_ratios, advantage_estimates, epsilon=epsilon)\n",
    "        total_loss = L_v + L_ppo\n",
    "            \n",
    "        optim.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch %d, steps: %d mean reward: %.3f, value loss: %.3f' % (epoch, steps, tensor_r.mean(), L_v.item()))\n",
    "    mean_rewards[epoch] = tensor_r.mean()\n",
    "    v_losses[epoch] = L_v.item()\n",
    "    p_losses[epoch] = L_ppo.item()\n",
    "    # raise\n",
    "        \n",
    "# train_env.close()\n",
    "\n",
    "plot_training(mean_rewards, p_losses, v_losses)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(pi.state_dict(), 'model_with_one_tanh.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "import pygame as pg \n",
    "from robots import CarLikeBot, CircleRobot\n",
    "from random import uniform\n",
    "from env import Environment\n",
    "from policies import ContiniousPolicy, DiscterePolicy\n",
    "from rewards import RewardCircle\n",
    "import time\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "from robots import CarLikeBot, CircleRobot\n",
    "from random import uniform\n",
    "from env import Environment\n",
    "from policies import ContiniousPolicy, DiscterePolicy\n",
    "from algo.ppo import PPO\n",
    "from rewards import RewardCircle\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "\n",
    "def sample_trajectories(env, pi, max_steps, i_map):    \n",
    "    ol = [[] for _ in range(len(env.agents))]\n",
    "    od = [[] for _ in range(len(env.agents))]\n",
    "    og = [[] for _ in range(len(env.agents))]\n",
    "    ov = [[] for _ in range(len(env.agents))]\n",
    "    actions = [[] for _ in range(len(env.agents))]  # actions from a(0) to a(T)\n",
    "    rewards = [[] for _ in range(len(env.agents))]  # rewards from r(0) to r(T)\n",
    "    env.reset(i_map)\n",
    "    env.get_observation()\n",
    "    \n",
    "    for index, ag in enumerate(env.agents):\n",
    "        if not ag.collision_w and not ag.collision_a and  not ag.reached:\n",
    "                ol_, od_, og_, ov_ = ag.history.get_vectors()\n",
    "                ol[index].append(ol_)\n",
    "                od[index].append(od_)\n",
    "                og[index].append(og_)\n",
    "                ov[index].append(ov_)\n",
    "#     print(len(od), len(od[0]), len(od[1]), len(actions), len(actions[0]), len(actions[1]))\n",
    "    indexes = []\n",
    "    for _ in range(max_steps):\n",
    "        # print(len(od), len(od[0]), len(od[1]), len(actions), len(actions[0]), len(actions[1]))\n",
    "        for index, ag in enumerate(env.agents):\n",
    "                if not ag.collision_w and not ag.collision_a and  not ag.reached:\n",
    "                        ol_, od_, og_, ov_ = ag.history.get_vectors()\n",
    "                        act_ = pi.sample_actions(\n",
    "                                torch.tensor(np.reshape(ol_, (1, 1, 64))).float(),\n",
    "                                torch.tensor(np.reshape(od_, (1, 4))).float(),\n",
    "                                torch.tensor(np.reshape(og_, (1, 8))).float(),\n",
    "                                torch.tensor(np.reshape(ov_, (1, 8))).float()\n",
    "                                )\n",
    "                        actions[index].append(act_)\n",
    "        \n",
    "        for index, ag in enumerate(env.agents):\n",
    "                if not ag.collision_w and not ag.collision_a and not ag.reached:\n",
    "                        ol_, od_, og_, ov_ = ag.history.get_vectors()\n",
    "                        ol[index].append(ol_)\n",
    "                        od[index].append(od_)\n",
    "                        og[index].append(og_)\n",
    "                        ov[index].append(ov_)\n",
    "                        \n",
    "        rew, done = env.step(np.array([a[-1] for a in actions]))\n",
    "        \n",
    "        for index, ag in enumerate(env.agents):\n",
    "                if index not in indexes:\n",
    "                        if ag.collision_w or ag.collision_a or ag.reached:\n",
    "                                indexes.append(index)\n",
    "                        rewards[index].append(rew[index])\n",
    "                        \n",
    "        if done:\n",
    "                break\n",
    "\n",
    "    for i in range(len(env.agents)):\n",
    "        ol[i] = torch.tensor(ol[i]).float()\n",
    "        od[i] = torch.tensor(od[i]).float()\n",
    "        og[i] = torch.tensor(og[i]).float()\n",
    "        ov[i] = torch.tensor(ov[i]).float()\n",
    "        actions[i] = torch.tensor(actions[i]).float()\n",
    "        rewards[i] = torch.tensor(rewards[i]).float()\n",
    "    \n",
    "    return ol, od, og, ov, actions, rewards\n",
    "\n",
    "def discount_cum_sum(rewards, gamma):\n",
    "    T = rewards.shape[0]\n",
    "    returns = torch.zeros_like(rewards)\n",
    "    for t in range(T):\n",
    "        # Compute the discounted sum of future rewards from time step t\n",
    "        returns[t] = torch.sum(rewards[t:] * (gamma ** torch.arange(T - t)))\n",
    "    return returns\n",
    "\n",
    "def compute_gae(tensor_r, values, gamma, lambda_):\n",
    "    delta_t = tensor_r + gamma * values[1:] - values[:-1]\n",
    "    advantages = discount_cum_sum(delta_t, gamma * lambda_)\n",
    "    value_targets = advantages + values[:-1]\n",
    "    return value_targets, advantages\n",
    "\n",
    "def value_loss(values, value_targets):\n",
    "    value_loss = torch.tensor(0.)\n",
    "    T = values.shape[0]\n",
    "    value_loss += (1/T)*torch.sum((values - value_targets) ** 2)\n",
    "    return value_loss\n",
    "\n",
    "def ppo_loss(p_ratios, advantage_estimates, epsilon):\n",
    "    policy_loss = torch.tensor(0.) \n",
    "    T = p_ratios.shape[0]\n",
    "    \n",
    "    p_ratios = p_ratios.sum(dim=1)\n",
    "    clipped_ratios = torch.clamp(p_ratios, 1.0 - epsilon, 1.0 + epsilon)\n",
    "\n",
    "    surrogate_loss = torch.min(p_ratios * advantage_estimates, clipped_ratios * advantage_estimates)\n",
    "    policy_loss -= (1/T)*torch.sum(surrogate_loss)\n",
    "    return policy_loss\n",
    "    \n",
    "def plot_training(rewards, p_losses, v_losses=None):\n",
    "    num_plots = 2 if v_losses is None else 3\n",
    "\n",
    "    plt.subplot(num_plots, 1, 1)\n",
    "    plt.plot(rewards, label='mean rewards', color='green')\n",
    "    plt.ylabel('Mean reward')\n",
    "    plt.subplot(num_plots, 1, 2)\n",
    "    plt.plot(p_losses, label='policy loss', color='red')\n",
    "    plt.ylabel('Policy loss')\n",
    "    if v_losses is not None:\n",
    "        plt.subplot(num_plots, 1, 3)\n",
    "        plt.plot(v_losses, label='value loss', color='blue')\n",
    "        plt.ylabel('Value loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.show()\n",
    "\n",
    "def compute_advanteges(rewards, values):\n",
    "        advantages = rewards - values\n",
    "        return advantages\n",
    "\n",
    "\n",
    "def PPO(\n",
    "        train_env,\n",
    "        pi,\n",
    "        path: str | None = None,\n",
    "        max_steps = 512,\n",
    "        epochs = 5000,\n",
    "        lr = 0.0003,\n",
    "        gamma=0.95,\n",
    "        epsilon = 0.2,\n",
    "        sgd_iters = 10,\n",
    "        maps = [4, 5, 6]\n",
    "        ):\n",
    "\n",
    "        if path:\n",
    "                pi.load_state_dict(torch.load(path))\n",
    "        optim = torch.optim.Adam(pi.parameters(), lr=lr)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "                ol, od, og, ov, actions, rewards = sample_trajectories(train_env, pi, max_steps, int(np.random.choice(maps)))\n",
    "                num_ag = len(ol)\n",
    "                with torch.no_grad():\n",
    "                        logp_old = []\n",
    "                        for i in range(num_ag):\n",
    "                                logp_old.append(pi.log_prob(\n",
    "                                        actions[i][:actions[i].shape[0]],\n",
    "                                        ol[i][:actions[i].shape[0]],\n",
    "                                        od[i][:actions[i].shape[0]],\n",
    "                                        og[i][:actions[i].shape[0]],\n",
    "                                        ov[i][:actions[i].shape[0]]\n",
    "                                        ))\n",
    "\n",
    "                for _ in range(sgd_iters):\n",
    "                        values = []\n",
    "                        for i in range(num_ag):\n",
    "                                values.append(pi.value_estimates(\n",
    "                                        ol[i][:actions[i].shape[0]+1],\n",
    "                                        od[i][:actions[i].shape[0]+1],\n",
    "                                        og[i][:actions[i].shape[0]+1],\n",
    "                                        ov[i][:actions[i].shape[0]+1]\n",
    "                                ))\n",
    "\n",
    "                        logp = []\n",
    "                        for i in range(num_ag):\n",
    "                                logp.append(pi.log_prob(\n",
    "                                        actions[i][:actions[i].shape[0]],\n",
    "                                        ol[i][:actions[i].shape[0]],\n",
    "                                        od[i][:actions[i].shape[0]],\n",
    "                                        og[i][:actions[i].shape[0]],\n",
    "                                        ov[i][:actions[i].shape[0]]\n",
    "                                ))\n",
    "\n",
    "                        with torch.no_grad():\n",
    "                                value_targets, advantage_estimates = [], []\n",
    "                                for i in range(num_ag):\n",
    "                                        value_t, advantage_e = compute_gae(\n",
    "                                                rewards[i][:actions[i].shape[0]],\n",
    "                                                values[i][:actions[i].shape[0]+1],\n",
    "                                                gamma, lambda_=0.97)\n",
    "                                        advantage_e = (advantage_e - advantage_e.mean()) / advantage_e.std()\n",
    "                                        value_targets.append(value_t)\n",
    "                                        advantage_estimates.append(advantage_e)\n",
    "                        \n",
    "                        for i in range(num_ag):        \n",
    "                                L_v = value_loss(values[i][:actions[i].shape[0]], value_targets[i])\n",
    "                                \n",
    "                                p_ratios = torch.exp(logp[i] - logp_old[i])\n",
    "                        \n",
    "                                L_ppo = ppo_loss(p_ratios, advantage_estimates[i], epsilon=epsilon)\n",
    "                                total_loss = L_v + L_ppo\n",
    "                                \n",
    "                                optim.zero_grad()\n",
    "                                total_loss.backward()\n",
    "                                optim.step()\n",
    "                        \n",
    "                if epoch % 100 == 0:\n",
    "                        print('Epoch %d' % (epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc = True\n",
    "maps_ = [4, 5, 6, 7]\n",
    "\n",
    "env = Environment(CarLikeBot, int(np.random.choice(maps_)), reward=RewardCircle(), discrete=disc)\n",
    "if disc:\n",
    "    p = DiscterePolicy()\n",
    "else: \n",
    "    p = ContiniousPolicy()\n",
    "    p.load_state_dict(torch.load('pretrained_models/model_with_one_tanh.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kajratbekbolinov/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:266: UserWarning: Error detected in AddmmBackward0. Traceback of forward call that caused the error:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/kajratbekbolinov/anaconda3/lib/python3.11/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/kajratbekbolinov/anaconda3/lib/python3.11/site-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/kajratbekbolinov/anaconda3/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 736, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/kajratbekbolinov/anaconda3/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Users/kajratbekbolinov/anaconda3/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Users/kajratbekbolinov/anaconda3/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Users/kajratbekbolinov/anaconda3/lib/python3.11/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/kajratbekbolinov/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/kajratbekbolinov/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/kajratbekbolinov/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/kajratbekbolinov/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/kajratbekbolinov/anaconda3/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/kajratbekbolinov/anaconda3/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/kajratbekbolinov/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3024, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/kajratbekbolinov/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3079, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/kajratbekbolinov/anaconda3/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/kajratbekbolinov/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3284, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/kajratbekbolinov/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3466, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/kajratbekbolinov/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/31/q99yh7r911j4j8l175mrzyjh0000gn/T/ipykernel_18346/203790096.py\", line 1, in <module>\n",
      "    PPO(\n",
      "  File \"/var/folders/31/q99yh7r911j4j8l175mrzyjh0000gn/T/ipykernel_18346/2470807610.py\", line 170, in PPO\n",
      "    logp.append(pi.log_prob(\n",
      "  File \"/Users/kajratbekbolinov/Desktop/mutlti_robots/policies.py\", line 222, in log_prob\n",
      "    params = self.forward(ol, od, og, ov)\n",
      "  File \"/Users/kajratbekbolinov/Desktop/mutlti_robots/policies.py\", line 169, in forward\n",
      "    velocity = self.fc_velocity(merged)\n",
      "  File \"/Users/kajratbekbolinov/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/Users/kajratbekbolinov/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/kajratbekbolinov/anaconda3/lib/python3.11/site-packages/torch/nn/modules/container.py\", line 217, in forward\n",
      "    input = module(input)\n",
      "  File \"/Users/kajratbekbolinov/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/Users/kajratbekbolinov/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/kajratbekbolinov/anaconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py\", line 116, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      " (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:118.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [264, 10]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m PPO(\n\u001b[1;32m      2\u001b[0m     env,\n\u001b[1;32m      3\u001b[0m     p,\n\u001b[1;32m      4\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m      5\u001b[0m     max_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m,\n\u001b[1;32m      6\u001b[0m     epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5000\u001b[39m,\n\u001b[1;32m      7\u001b[0m     lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0003\u001b[39m,\n\u001b[1;32m      8\u001b[0m     gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m,\n\u001b[1;32m      9\u001b[0m     epsilon \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m,\n\u001b[1;32m     10\u001b[0m     sgd_iters \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     11\u001b[0m     maps \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m6\u001b[39m]\n\u001b[1;32m     12\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[25], line 198\u001b[0m, in \u001b[0;36mPPO\u001b[0;34m(train_env, pi, path, max_steps, epochs, lr, gamma, epsilon, sgd_iters, maps)\u001b[0m\n\u001b[1;32m    195\u001b[0m                 total_loss \u001b[38;5;241m=\u001b[39m L_v \u001b[38;5;241m+\u001b[39m L_ppo\n\u001b[1;32m    197\u001b[0m                 optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 198\u001b[0m                 total_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    199\u001b[0m                 optim\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    524\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    267\u001b[0m     tensors,\n\u001b[1;32m    268\u001b[0m     grad_tensors_,\n\u001b[1;32m    269\u001b[0m     retain_graph,\n\u001b[1;32m    270\u001b[0m     create_graph,\n\u001b[1;32m    271\u001b[0m     inputs,\n\u001b[1;32m    272\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    273\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    274\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [264, 10]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!"
     ]
    }
   ],
   "source": [
    "PPO(\n",
    "    env,\n",
    "    p,\n",
    "    path = None,\n",
    "    max_steps = 256,\n",
    "    epochs = 5000,\n",
    "    lr = 0.0003,\n",
    "    gamma=0.95,\n",
    "    epsilon = 0.2,\n",
    "    sgd_iters = 10,\n",
    "    maps = [4, 5, 6]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1 1 2 0 0\n",
      "2 1 1 2 0 0\n",
      "2 2 2 2 1 1\n",
      "2 3 3 2 2 2\n",
      "2 4 4 2 3 3\n",
      "2 5 5 2 4 4\n",
      "2 6 6 2 5 5\n",
      "2 7 7 2 6 6\n",
      "2 8 8 2 7 7\n",
      "2 9 9 2 8 8\n",
      "2 10 10 2 9 9\n",
      "2 11 11 2 10 10\n",
      "2 12 12 2 11 11\n",
      "2 13 13 2 12 12\n",
      "2 14 14 2 13 13\n",
      "2 15 15 2 14 14\n",
      "2 16 16 2 15 15\n",
      "2 17 17 2 16 16\n",
      "2 18 18 2 17 17\n",
      "2 19 19 2 18 18\n",
      "2 20 20 2 19 19\n",
      "2 21 21 2 20 20\n",
      "2 22 22 2 21 21\n",
      "2 23 23 2 22 22\n",
      "2 24 24 2 23 23\n",
      "2 25 25 2 24 24\n",
      "2 25 26 2 24 25\n",
      "2 25 27 2 24 26\n",
      "2 25 28 2 24 27\n",
      "2 25 29 2 24 28\n",
      "2 25 30 2 24 29\n"
     ]
    }
   ],
   "source": [
    "ol, od, og, ov, actions, rewards = sample_trajectories(env, p, 40, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, torch.Size([25, 1, 64]), torch.Size([30, 1, 64]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ol), ol[0].shape, ol[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, torch.Size([25, 4]), torch.Size([30, 4]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(od), od[0].shape, od[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, torch.Size([25, 8]), torch.Size([30, 8]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(og), og[0].shape, og[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, torch.Size([24, 2]), torch.Size([29, 2]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(actions), actions[0].shape, actions[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, torch.Size([24]), torch.Size([29]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rewards), rewards[0].shape, rewards[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.7500), tensor(-0.7500))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards[0][-1], rewards[1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[0].shape[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
